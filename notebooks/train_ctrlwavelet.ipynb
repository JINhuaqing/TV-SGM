{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3785c4-9f12-48d6-b30b-543b5d0a2f85",
   "metadata": {},
   "source": [
    "Now I run the real data (Ctrl data in AD_vs_Ctrl dataset)\n",
    "\n",
    "Convert to dB Scale\n",
    "\n",
    "- 由于现在我处理的ctrl的spectrogram的freqs pts和36-MEG data 一样，所以你可以用SGM_net来simulate PSD。\n",
    "- 但是，顺序是不一样的，我用的是36-MEG data的顺序训练的这个net。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77566fcf-6190-41eb-8168-972aeea4fcad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T07:39:21.958704Z",
     "iopub.status.busy": "2024-01-05T07:39:21.958130Z",
     "iopub.status.idle": "2024-01-05T07:39:21.981065Z",
     "shell.execute_reply": "2024-01-05T07:39:21.980095Z",
     "shell.execute_reply.started": "2024-01-05T07:39:21.958642Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RUN_PYTHON_SCRIPT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba9942dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T17:37:57.771920Z",
     "start_time": "2023-04-04T17:37:56.085736Z"
    },
    "execution": {
     "iopub.execute_input": "2024-01-05T07:39:21.983174Z",
     "iopub.status.busy": "2024-01-05T07:39:21.982456Z",
     "iopub.status.idle": "2024-01-05T07:39:22.121060Z",
     "shell.execute_reply": "2024-01-05T07:39:22.120333Z",
     "shell.execute_reply.started": "2024-01-05T07:39:21.983130Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../mypkg\")\n",
    "from constants import RES_ROOT, FIG_ROOT, DATA_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad09a305",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T17:38:03.434191Z",
     "start_time": "2023-04-04T17:37:58.883670Z"
    },
    "execution": {
     "iopub.execute_input": "2024-01-05T07:39:22.122586Z",
     "iopub.status.busy": "2024-01-05T07:39:22.122247Z",
     "iopub.status.idle": "2024-01-05T07:39:22.934542Z",
     "shell.execute_reply": "2024-01-05T07:39:22.933553Z",
     "shell.execute_reply.started": "2024-01-05T07:39:22.122569Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from easydict import EasyDict as edict\n",
    "from tqdm import trange, tqdm\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "plt.style.use(FIG_ROOT/\"base.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef25fde2-bcc1-402d-935e-9c4053a76696",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T07:39:22.935855Z",
     "iopub.status.busy": "2024-01-05T07:39:22.935336Z",
     "iopub.status.idle": "2024-01-05T07:39:22.952979Z",
     "shell.execute_reply": "2024-01-05T07:39:22.952481Z",
     "shell.execute_reply.started": "2024-01-05T07:39:22.935832Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# 0,1, 2, 3, be careful about the space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0924abca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T17:38:12.656580Z",
     "start_time": "2023-04-04T17:38:12.479288Z"
    },
    "execution": {
     "iopub.execute_input": "2024-01-05T07:39:22.953865Z",
     "iopub.status.busy": "2024-01-05T07:39:22.953698Z",
     "iopub.status.idle": "2024-01-05T07:39:24.227933Z",
     "shell.execute_reply": "2024-01-05T07:39:24.226776Z",
     "shell.execute_reply.started": "2024-01-05T07:39:22.953851Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.reparam import theta2raw_torch, raw2theta_torch, raw2theta_np\n",
    "from spectrome import Brain\n",
    "from sgm.sgm import SGM\n",
    "from utils.stable import paras_stable_check\n",
    "from utils.misc import save_pkl, save_pkl_dict2folder, load_pkl, load_pkl_folder2dict, delta_time\n",
    "from models.lstm import LSTM_SGM\n",
    "from models.loss import  weighted_mse_loss, reg_R_loss, lin_R_loss, lin_R_fn, reg_R_fn\n",
    "from models.model_utils import weights_init\n",
    "from utils.standardize import std_mat, std_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "318c4ef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T17:38:12.665771Z",
     "start_time": "2023-04-04T17:38:12.659314Z"
    },
    "execution": {
     "iopub.execute_input": "2024-01-05T07:39:24.229509Z",
     "iopub.status.busy": "2024-01-05T07:39:24.229064Z",
     "iopub.status.idle": "2024-01-05T07:39:24.254826Z",
     "shell.execute_reply": "2024-01-05T07:39:24.254227Z",
     "shell.execute_reply.started": "2024-01-05T07:39:24.229480Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pkgs for pytorch ( Mar 27, 2023) \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.functional import F\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "df_dtype = torch.float32\n",
    "torch.set_default_dtype(df_dtype)\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device(\"cuda\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    torch.set_default_device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62334fad-1a0e-4a41-9f1e-5595f85fbed4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T07:39:24.255594Z",
     "iopub.status.busy": "2024-01-05T07:39:24.255439Z",
     "iopub.status.idle": "2024-01-05T07:39:24.279661Z",
     "shell.execute_reply": "2024-01-05T07:39:24.279241Z",
     "shell.execute_reply.started": "2024-01-05T07:39:24.255582Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed)\n",
    "torch.use_deterministic_algorithms(True); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42cd127-34b0-4132-a0a3-efa7f5c514cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d7a4da6",
   "metadata": {},
   "source": [
    "# Data, fn and paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3592408-44fd-4554-870f-b42938765231",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T07:39:24.280372Z",
     "iopub.status.busy": "2024-01-05T07:39:24.280220Z",
     "iopub.status.idle": "2024-01-05T07:39:24.319361Z",
     "shell.execute_reply": "2024-01-05T07:39:24.318732Z",
     "shell.execute_reply.started": "2024-01-05T07:39:24.280359Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get order idx\n",
    "# idx from 36-MEG order to Ctrl-data order\n",
    "# org_order: ctrl data order\n",
    "# target_order: SGMnet order, SC order\n",
    "org_order = np.loadtxt(DATA_ROOT/\"AD_vs_Ctrl_ts/roi_order.txt\", dtype=str);\n",
    "target_order0 = np.loadtxt(DATA_ROOT/\"DK_atlas_36MEG.txt\", dtype=str);\n",
    "target_order = np.array([f\"ctx-{roi_name.split('_')[1].lower()}h-{roi_name.split('_')[0].lower()}\" for roi_name in target_order0[:68]]);\n",
    "org2target_idxs = np.array([np.where(org_order==roi)[0][0] for roi in target_order]);\n",
    "(org_order[org2target_idxs] == target_order).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bf14395-dc0a-4af2-a2e2-bd504d7818d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T07:39:24.320445Z",
     "iopub.status.busy": "2024-01-05T07:39:24.320128Z",
     "iopub.status.idle": "2024-01-05T07:39:25.651510Z",
     "shell.execute_reply": "2024-01-05T07:39:25.650975Z",
     "shell.execute_reply.started": "2024-01-05T07:39:24.320423Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load file /data/rajlab1/user_data/jin/MyResearch/TV-SGM/notebooks/../mypkg/../data/AD_vs_Ctrl_ts/spectrogram_wavelet100.pkl\n"
     ]
    }
   ],
   "source": [
    "psd_all_full0 = load_pkl(DATA_ROOT/\"AD_vs_Ctrl_ts/spectrogram_wavelet100.pkl\");\n",
    "\n",
    "psd_all_full = np.array([res[\"spectrogram\"] for res in psd_all_full0]);\n",
    "psd_all_full = 10 * np.log10(psd_all_full) # to dB scale\n",
    "# make the ctrl data order is compatible to SGM net \n",
    "psd_all_full = psd_all_full[:, org2target_idxs] # num_sub x num_roi x num_freqs x num_ts\n",
    "time_points = psd_all_full0[0][\"times\"]\n",
    "freqs = psd_all_full0[0][\"freqs\"];\n",
    "#I remove the first and last time pts\n",
    "rm_lim = 5\n",
    "if rm_lim > 0:\n",
    "    psd_all_full = psd_all_full[:, :, :, rm_lim:-rm_lim]\n",
    "    time_points = time_points[rm_lim:-rm_lim];"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35923f9b-6f90-471c-b550-442a763ff4a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T00:18:51.515335Z",
     "iopub.status.busy": "2024-01-05T00:18:51.514553Z",
     "iopub.status.idle": "2024-01-05T00:19:30.762185Z",
     "shell.execute_reply": "2024-01-05T00:19:30.760953Z",
     "shell.execute_reply.started": "2024-01-05T00:18:51.515289Z"
    },
    "tags": []
   },
   "source": [
    "t_ixs = [0, 1, 2, 10, 100, 200, -2, -1]\n",
    "all_corrss = []\n",
    "for t_ix in tqdm(t_ixs):\n",
    "    all_corrs = []\n",
    "    for sub_ix in range(92):\n",
    "        cor_seq = []\n",
    "        for roi_ix in range(68):\n",
    "            corrmat = np.corrcoef(psd_all_full[sub_ix, roi_ix].T)\n",
    "            cor_seq.append(corrmat[t_ix, :].mean())\n",
    "        all_corrs.append(cor_seq)\n",
    "    all_corrs = np.array(all_corrs);    \n",
    "    all_corrss.append(all_corrs)\n",
    "    \n",
    "plt.figure(figsize=[10, 5])\n",
    "for idx in range(len(t_ixs)):\n",
    "    plt.subplot(2, 4, idx+1)\n",
    "    \n",
    "    plt.title(f\"Time {t_ixs[idx]}\")\n",
    "    sns.heatmap(all_corrss[idx], vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f317c230-635a-462b-8a90-f8e8a5dfc0fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "180f0d16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T17:38:18.614858Z",
     "start_time": "2023-04-04T17:38:18.509968Z"
    },
    "execution": {
     "iopub.execute_input": "2024-01-05T07:39:25.652267Z",
     "iopub.status.busy": "2024-01-05T07:39:25.652101Z",
     "iopub.status.idle": "2024-01-05T07:39:25.681728Z",
     "shell.execute_reply": "2024-01-05T07:39:25.681284Z",
     "shell.execute_reply.started": "2024-01-05T07:39:25.652254Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the Connectome\n",
    "brain = Brain.Brain()\n",
    "brain.add_connectome(DATA_ROOT)\n",
    "brain.reorder_connectome(brain.connectome, brain.distance_matrix)\n",
    "brain.bi_symmetric_c()\n",
    "brain.reduce_extreme_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f85a7c05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T17:38:22.084241Z",
     "start_time": "2023-04-04T17:38:22.076317Z"
    },
    "execution": {
     "iopub.execute_input": "2024-01-05T07:39:25.682501Z",
     "iopub.status.busy": "2024-01-05T07:39:25.682340Z",
     "iopub.status.idle": "2024-01-05T07:39:25.700956Z",
     "shell.execute_reply": "2024-01-05T07:39:25.700539Z",
     "shell.execute_reply.started": "2024-01-05T07:39:25.682487Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# some constant parameters for this file\n",
    "paras = edict()\n",
    "\n",
    "## I reorder them in an alphabetical order and I change tauC to tauG (Mar 27, 2023)\n",
    "## the orginal order is taue, taui, tauC, speed, alpha, gii, gei\n",
    "## paras.par_low = np.asarray([0.005,0.005,0.005,5, 0.1,0.001,0.001])\n",
    "## paras.par_high = np.asarray([0.03, 0.20, 0.03,20,  1,    2,  0.7])\n",
    "##\n",
    "\n",
    "# alpha, gei, gii, taue, tauG, taui, speed \n",
    "paras.par_low = np.array([0.1, 0.001,0.001, 0.005, 0.005, 0.005, 5])\n",
    "paras.par_high = np.asarray([1, 0.7, 2, 0.03, 0.03, 0.20, 20])\n",
    "paras.prior_bds = np.array([paras.par_low, paras.par_high]).T\n",
    "paras.names = [\"alpha\", \"gei\", \"gii\", \"Taue\", \"TauG\", \"Taui\", \"Speed\"]\n",
    "\n",
    "paras.C = brain.reducedConnectome\n",
    "paras.D = brain.distance_matrix\n",
    "paras.freqs = freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1296b9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65ce16ba",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b49a3544-8248-45e0-b006-4464dbaaf09f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T17:38:36.267294Z",
     "start_time": "2023-04-04T17:38:35.514912Z"
    },
    "execution": {
     "iopub.execute_input": "2024-01-05T07:39:25.702635Z",
     "iopub.status.busy": "2024-01-05T07:39:25.702437Z",
     "iopub.status.idle": "2024-01-05T07:39:25.773230Z",
     "shell.execute_reply": "2024-01-05T07:39:25.772640Z",
     "shell.execute_reply.started": "2024-01-05T07:39:25.702622Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load file /data/rajlab1/user_data/jin/MyResearch/TV-SGM/notebooks/../mypkg/../results/SGM_net/freqs.pkl\n",
      "Load file /data/rajlab1/user_data/jin/MyResearch/TV-SGM/notebooks/../mypkg/../results/SGM_net/loss.pkl\n",
      "Load file /data/rajlab1/user_data/jin/MyResearch/TV-SGM/notebooks/../mypkg/../results/SGM_net/loss_test.pkl\n",
      "Load file /data/rajlab1/user_data/jin/MyResearch/TV-SGM/notebooks/../mypkg/../results/SGM_net/model.pkl\n",
      "Load file /data/rajlab1/user_data/jin/MyResearch/TV-SGM/notebooks/../mypkg/../results/SGM_net/paras.pkl\n"
     ]
    }
   ],
   "source": [
    "trained_model = load_pkl_folder2dict(RES_ROOT/\"SGM_net\", excluding=['opt*'])\n",
    "sgm_net = trained_model.model;\n",
    "sgm_net.to(dtype=df_dtype);\n",
    "sgm_net.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fe17f7e-27c4-46fb-8c28-37e34b229746",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T07:39:25.774179Z",
     "iopub.status.busy": "2024-01-05T07:39:25.774000Z",
     "iopub.status.idle": "2024-01-05T07:39:25.792096Z",
     "shell.execute_reply": "2024-01-05T07:39:25.791642Z",
     "shell.execute_reply.started": "2024-01-05T07:39:25.774165Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not RUN_PYTHON_SCRIPT:\n",
    "    plt.plot(np.array(trained_model.loss)/10, label=\"train loss\")\n",
    "    plt.plot(trained_model.loss_test, label=\"test loss\")\n",
    "    #plt.xticks(np.arange(0, 120, 14));\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d507dc44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T17:38:47.060338Z",
     "start_time": "2023-04-04T17:38:46.969000Z"
    },
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2024-01-05T07:39:25.792888Z",
     "iopub.status.busy": "2024-01-05T07:39:25.792653Z",
     "iopub.status.idle": "2024-01-05T07:39:25.893562Z",
     "shell.execute_reply": "2024-01-05T07:39:25.893159Z",
     "shell.execute_reply.started": "2024-01-05T07:39:25.792867Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# functions to generate training sample (Apr 1, 2023)\n",
    "def random_choice(n, batchsize=1, len_seg=None):\n",
    "    \"\"\"Randomly select the lower and upper bound of the segment\n",
    "        args:\n",
    "            n: len of the total time series\n",
    "    \"\"\"\n",
    "    if len_seg is None:\n",
    "        len_seg = torch.randint(low=10, high=100, size=(1, ))\n",
    "    up_bd = torch.randint(low=len_seg.item(), high=n, size=(batchsize, ))\n",
    "    low_bd = up_bd - len_seg\n",
    "    return low_bd, up_bd\n",
    "\n",
    "\n",
    "def random_samples_rnn(X, Y=None, batchsize=1, \n",
    "                       bds=None, \n",
    "                       is_std=True, \n",
    "                       theta2raw_fn=None):\n",
    "    \"\"\"Randomly select a sample from the whole segment\n",
    "        args:\n",
    "            X: PSD, num_seq x 68 x nfreq or \n",
    "               PSD, num_sub x num_seq x 68 x nfreq\n",
    "            Y: params, num x 7, in original sgm scale\n",
    "        return:\n",
    "            X_seqs: len_seq x batchsize x num_fs\n",
    "            Y_seqs: len_seq x batchsize x 7\n",
    "            \n",
    "    \"\"\"\n",
    "    if X.ndim == 4:\n",
    "        # if multiple subjects, pick up a subject\n",
    "        num_sub = X.shape[0]\n",
    "        sub_idx = np.random.randint(low=0, high=num_sub)\n",
    "        X = X[sub_idx]\n",
    "        \n",
    "    if not isinstance(X, torch.Tensor):\n",
    "        X = torch.tensor(X)\n",
    "    if is_std:\n",
    "        #X = X/X.std(axis=(1, 2), keepdims=True)\n",
    "        # Let std for each ROI and each data\n",
    "        X = (X-X.mean(axis=2, keepdims=True))/X.std(axis=2, keepdims=True)\n",
    "    if Y is not None:\n",
    "        if not isinstance(Y, torch.Tensor):\n",
    "            Y = torch.tensor(Y)\n",
    "        if theta2raw_fn: \n",
    "            Y = theta2raw_fn(Y)\n",
    "    if bds is None:\n",
    "        low_bds, up_bds = random_choice(len(X), batchsize)\n",
    "    else:\n",
    "        low_bds, up_bds = bds\n",
    "\n",
    "    X = X.flatten(1)\n",
    "    X_seqs = []\n",
    "    Y_seqs = []\n",
    "    for low_bd, up_bd in zip(low_bds, up_bds):\n",
    "        X_seq = X[low_bd:up_bd, :].unsqueeze(1)\n",
    "        X_seqs.append(X_seq)\n",
    "        if Y is not None:\n",
    "            Y_seq = Y[low_bd:up_bd].unsqueeze(1)\n",
    "            Y_seqs.append(Y_seq)\n",
    "    if Y is not None:\n",
    "        return torch.cat(X_seqs, dim=1), torch.cat(Y_seqs, dim=1)\n",
    "    else:\n",
    "        return torch.cat(X_seqs, dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2db4078d-5415-4185-9076-db8bdcff80db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T07:39:25.894342Z",
     "iopub.status.busy": "2024-01-05T07:39:25.894094Z",
     "iopub.status.idle": "2024-01-05T07:39:25.917259Z",
     "shell.execute_reply": "2024-01-05T07:39:25.916912Z",
     "shell.execute_reply.started": "2024-01-05T07:39:25.894328Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _evaluate(all_data):\n",
    "    num_sub, len_seq, _, _ = all_data.shape\n",
    "    all_data_raw = torch.tensor(all_data, dtype=df_dtype).transpose(1, 0)\n",
    "    all_data_input = (all_data_raw - all_data_raw.mean(axis=-1, keepdims=True))/all_data_raw.std(axis=-1, keepdims=True);\n",
    "    all_data_input = all_data_input.flatten(2);\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        Y_pred = rnn(all_data_input);\n",
    "        X_pred = sgm_net(Y_pred.flatten(0, 1));\n",
    "    corrs = reg_R_fn(all_data_raw.flatten(0, 1), X_pred);\n",
    "    corrs = corrs.reshape(len_seq, num_sub, -1).transpose(1, 0)\n",
    "    return corrs.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ace453aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T19:25:30.343060Z",
     "start_time": "2023-04-04T19:25:30.335835Z"
    },
    "execution": {
     "iopub.execute_input": "2024-01-05T07:39:25.917960Z",
     "iopub.status.busy": "2024-01-05T07:39:25.917778Z",
     "iopub.status.idle": "2024-01-05T07:39:26.490159Z",
     "shell.execute_reply": "2024-01-05T07:39:26.489707Z",
     "shell.execute_reply.started": "2024-01-05T07:39:25.917947Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paras_rnn = edict()\n",
    "# batchsize is not in fact used.\n",
    "paras_rnn.batchsize = 128\n",
    "paras_rnn.niter = 1000 #!!!! 500\n",
    "paras_rnn.loss_out = 1\n",
    "paras_rnn.eval_out = 20\n",
    "paras_rnn.clip = 1 # from \n",
    "paras_rnn.lr_step = 300 #!!!! 10\n",
    "paras_rnn.gamma = 0.5 #!!!! 0.9\n",
    "paras_rnn.lr = 2e-4 \n",
    "\n",
    "paras_rnn.k = 1\n",
    "paras_rnn.hidden_dim = int(1024/4)\n",
    "paras_rnn.output_dim = 7\n",
    "paras_rnn.input_dim = 68*len(paras.freqs)\n",
    "paras_rnn.is_bidirectional = False#!!!!False\n",
    "paras_rnn.unstable_pen = 10000 # Whether to filter out the unstable sps or not, if 0 not, if large number, yes\n",
    "paras_rnn.loss_name = \"wmse\" # linR, corr, wmse or mse\n",
    "#paras.names = [\"alpha\", \"gei\", \"gii\", \"Taue\", \"TauG\", \"Taui\", \"Speed\"]\n",
    "# 1 dynamic, 0 static\n",
    "paras_rnn.dy_mask = [1, 1, 1, 1, 1, 1, 0] \n",
    "stat_part = \"_\".join(np.array(paras.names)[np.array(paras_rnn.dy_mask)==0][:-1])\n",
    "if len(stat_part) > 0:\n",
    "    folder_name = f\"LSTM_simu_net_ctrlwavelet_{paras_rnn.loss_name}_{stat_part}\";\n",
    "else:\n",
    "    folder_name = f\"LSTM_simu_net_ctrlwavelet_{paras_rnn.loss_name}\";\n",
    "paras_rnn.save_dir = RES_ROOT/folder_name\n",
    "\n",
    "\n",
    "psd_all = psd_all_full\n",
    "#  all_data is the real data, should be num_sub x len_seq x nrois x nfreqs\n",
    "#  or len_seq x nrois x nfreqs\n",
    "all_data = psd_all.transpose(0, 3, 1, 2)\n",
    "\n",
    "all_data_raw = torch.tensor(all_data, dtype=df_dtype).transpose(1, 0)\n",
    "all_data_input = (all_data_raw - all_data_raw.mean(axis=-1, keepdims=True))/all_data_raw.std(axis=-1, keepdims=True);\n",
    "all_data_input = all_data_input.flatten(2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b8b59c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T20:27:57.475018Z",
     "start_time": "2023-04-04T20:27:57.334374Z"
    },
    "execution": {
     "iopub.execute_input": "2024-01-05T07:39:26.491002Z",
     "iopub.status.busy": "2024-01-05T07:39:26.490774Z",
     "iopub.status.idle": "2024-01-05T07:39:26.538321Z",
     "shell.execute_reply": "2024-01-05T07:39:26.537947Z",
     "shell.execute_reply.started": "2024-01-05T07:39:26.490987Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 2.0000e-04.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.optim.lr_scheduler.ExponentialLR at 0x7f5d73aaa8b0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = LSTM_SGM(input_dim=paras_rnn.input_dim, \n",
    "               hidden_dim=paras_rnn.hidden_dim, \n",
    "               output_dim=paras_rnn.output_dim, \n",
    "               is_bidirectional=paras_rnn.is_bidirectional, \n",
    "               prior_bds=torch.tensor(paras.prior_bds, dtype=df_dtype), \n",
    "               k = paras_rnn.k, \n",
    "               dy_mask = paras_rnn.dy_mask\n",
    ")\n",
    "rnn.apply(weights_init)\n",
    "rnn.to(dtype=df_dtype);\n",
    "if paras_rnn.loss_name.startswith(\"corr\"):\n",
    "    loss_fn = reg_R_loss\n",
    "elif paras_rnn.loss_name.startswith(\"linR\"):\n",
    "    loss_fn = lin_R_loss\n",
    "elif paras_rnn.loss_name.startswith(\"wmse\"):\n",
    "    loss_fn = weighted_mse_loss\n",
    "elif paras_rnn.loss_name.startswith(\"mse\"):\n",
    "    loss_fn = nn.MSELoss()\n",
    "else:\n",
    "    raise KeyError(\"No such loss\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(rnn.parameters(), lr=paras_rnn.lr, weight_decay=0)\n",
    "scheduler = ExponentialLR(optimizer, gamma=paras_rnn.gamma, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821baed1-26bf-4b83-8515-cec5db3a9d6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T20:39:31.112388Z",
     "start_time": "2023-04-04T20:27:58.069870Z"
    },
    "execution": {
     "iopub.execute_input": "2024-01-05T07:39:26.539183Z",
     "iopub.status.busy": "2024-01-05T07:39:26.538905Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iter 1/1000, the losses are 4.09199 (train). The time used is 3.838s. \n",
      "At iter 2/1000, the losses are 3.85951 (train). The time used is 4.601s. \n",
      "At iter 3/1000, the losses are 3.74026 (train). The time used is 4.535s. \n",
      "At iter 4/1000, the losses are 3.63380 (train). The time used is 4.487s. \n",
      "At iter 5/1000, the losses are 3.53072 (train). The time used is 4.507s. \n",
      "At iter 6/1000, the losses are 3.44235 (train). The time used is 3.941s. \n",
      "At iter 7/1000, the losses are 3.35579 (train). The time used is 3.753s. \n",
      "At iter 8/1000, the losses are 3.27785 (train). The time used is 3.671s. \n",
      "At iter 9/1000, the losses are 3.17918 (train). The time used is 3.755s. \n",
      "At iter 10/1000, the losses are 3.06642 (train). The time used is 3.810s. \n",
      "At iter 11/1000, the losses are 2.97116 (train). The time used is 3.869s. \n",
      "At iter 12/1000, the losses are 2.87971 (train). The time used is 3.664s. \n",
      "At iter 13/1000, the losses are 2.78726 (train). The time used is 4.251s. \n",
      "At iter 14/1000, the losses are 2.70295 (train). The time used is 3.781s. \n",
      "At iter 15/1000, the losses are 2.58267 (train). The time used is 4.004s. \n",
      "At iter 16/1000, the losses are 2.46628 (train). The time used is 4.604s. \n",
      "At iter 17/1000, the losses are 2.36488 (train). The time used is 3.907s. \n",
      "At iter 18/1000, the losses are 2.26713 (train). The time used is 3.712s. \n",
      "At iter 19/1000, the losses are 2.17616 (train). The time used is 4.302s. \n",
      "At iter 20/1000, the losses are 2.07521 (train). The time used is 4.598s. \n",
      "====================================================================================================\n",
      "At iter 20/1000, the losses on all data are 0.74982. The time used is 1.809s. \n",
      "====================================================================================================\n",
      "At iter 21/1000, the losses are 1.97338 (train). The time used is 3.825s. \n",
      "At iter 22/1000, the losses are 1.87426 (train). The time used is 3.738s. \n",
      "At iter 23/1000, the losses are 1.79936 (train). The time used is 3.895s. \n",
      "At iter 24/1000, the losses are 1.73593 (train). The time used is 4.291s. \n",
      "At iter 25/1000, the losses are 1.67124 (train). The time used is 4.686s. \n",
      "At iter 26/1000, the losses are 1.62093 (train). The time used is 4.578s. \n",
      "At iter 27/1000, the losses are 1.59704 (train). The time used is 3.922s. \n",
      "At iter 28/1000, the losses are 1.58489 (train). The time used is 3.829s. \n",
      "At iter 29/1000, the losses are 1.55318 (train). The time used is 3.779s. \n",
      "At iter 30/1000, the losses are 1.52098 (train). The time used is 3.766s. \n",
      "At iter 31/1000, the losses are 1.49794 (train). The time used is 4.496s. \n",
      "At iter 32/1000, the losses are 1.48188 (train). The time used is 4.346s. \n",
      "At iter 33/1000, the losses are 1.46942 (train). The time used is 3.733s. \n",
      "At iter 34/1000, the losses are 1.45928 (train). The time used is 3.726s. \n",
      "At iter 35/1000, the losses are 1.44635 (train). The time used is 4.004s. \n",
      "At iter 36/1000, the losses are 1.43454 (train). The time used is 3.724s. \n",
      "At iter 37/1000, the losses are 1.42097 (train). The time used is 3.774s. \n",
      "At iter 38/1000, the losses are 1.40635 (train). The time used is 3.746s. \n",
      "At iter 39/1000, the losses are 1.39765 (train). The time used is 3.823s. \n",
      "At iter 40/1000, the losses are 1.39026 (train). The time used is 3.754s. \n",
      "====================================================================================================\n",
      "At iter 40/1000, the losses on all data are 0.82637. The time used is 1.417s. \n",
      "====================================================================================================\n",
      "At iter 41/1000, the losses are 1.38758 (train). The time used is 3.819s. \n",
      "At iter 42/1000, the losses are 1.38408 (train). The time used is 3.906s. \n",
      "At iter 43/1000, the losses are 1.38000 (train). The time used is 3.697s. \n",
      "At iter 44/1000, the losses are 1.37472 (train). The time used is 3.695s. \n",
      "At iter 45/1000, the losses are 1.37048 (train). The time used is 3.773s. \n",
      "At iter 46/1000, the losses are 1.36905 (train). The time used is 3.762s. \n",
      "At iter 47/1000, the losses are 1.36724 (train). The time used is 4.133s. \n",
      "At iter 48/1000, the losses are 1.36469 (train). The time used is 4.598s. \n",
      "At iter 49/1000, the losses are 1.36127 (train). The time used is 4.296s. \n",
      "At iter 50/1000, the losses are 1.35804 (train). The time used is 3.964s. \n",
      "At iter 51/1000, the losses are 1.35693 (train). The time used is 3.795s. \n",
      "At iter 52/1000, the losses are 1.35572 (train). The time used is 3.738s. \n",
      "At iter 53/1000, the losses are 1.35358 (train). The time used is 3.806s. \n",
      "At iter 54/1000, the losses are 1.35167 (train). The time used is 4.041s. \n",
      "At iter 55/1000, the losses are 1.35069 (train). The time used is 3.705s. \n",
      "At iter 56/1000, the losses are 1.34932 (train). The time used is 3.768s. \n",
      "At iter 57/1000, the losses are 1.34770 (train). The time used is 3.767s. \n",
      "At iter 58/1000, the losses are 1.34594 (train). The time used is 3.869s. \n",
      "At iter 59/1000, the losses are 1.34432 (train). The time used is 3.777s. \n",
      "At iter 60/1000, the losses are 1.34280 (train). The time used is 3.730s. \n",
      "====================================================================================================\n",
      "At iter 60/1000, the losses on all data are 0.82980. The time used is 1.415s. \n",
      "====================================================================================================\n",
      "At iter 61/1000, the losses are 1.34092 (train). The time used is 3.905s. \n",
      "At iter 62/1000, the losses are 1.33904 (train). The time used is 3.689s. \n",
      "At iter 63/1000, the losses are 1.33749 (train). The time used is 3.686s. \n",
      "At iter 64/1000, the losses are 1.33566 (train). The time used is 3.853s. \n",
      "At iter 65/1000, the losses are 1.33337 (train). The time used is 4.273s. \n",
      "At iter 66/1000, the losses are 1.33174 (train). The time used is 3.731s. \n",
      "At iter 67/1000, the losses are 1.32928 (train). The time used is 3.939s. \n",
      "At iter 68/1000, the losses are 1.32713 (train). The time used is 4.181s. \n",
      "At iter 69/1000, the losses are 1.32485 (train). The time used is 4.646s. \n",
      "At iter 70/1000, the losses are 1.32211 (train). The time used is 4.136s. \n",
      "At iter 71/1000, the losses are 1.31986 (train). The time used is 3.741s. \n",
      "At iter 72/1000, the losses are 1.31714 (train). The time used is 3.961s. \n",
      "At iter 73/1000, the losses are 1.31494 (train). The time used is 3.935s. \n",
      "At iter 74/1000, the losses are 1.31238 (train). The time used is 3.759s. \n",
      "At iter 75/1000, the losses are 1.31025 (train). The time used is 3.748s. \n",
      "At iter 76/1000, the losses are 1.30889 (train). The time used is 3.796s. \n",
      "At iter 77/1000, the losses are 1.30750 (train). The time used is 3.829s. \n",
      "At iter 78/1000, the losses are 1.30658 (train). The time used is 3.945s. \n",
      "At iter 79/1000, the losses are 1.30594 (train). The time used is 3.816s. \n",
      "At iter 80/1000, the losses are 1.30498 (train). The time used is 3.904s. \n",
      "====================================================================================================\n",
      "At iter 80/1000, the losses on all data are 0.83196. The time used is 1.823s. \n",
      "====================================================================================================\n",
      "At iter 81/1000, the losses are 1.30410 (train). The time used is 3.951s. \n",
      "At iter 82/1000, the losses are 1.30337 (train). The time used is 3.701s. \n",
      "At iter 83/1000, the losses are 1.30241 (train). The time used is 3.765s. \n",
      "At iter 84/1000, the losses are 1.30154 (train). The time used is 3.907s. \n",
      "At iter 85/1000, the losses are 1.30070 (train). The time used is 3.787s. \n",
      "At iter 86/1000, the losses are 1.29989 (train). The time used is 3.972s. \n",
      "At iter 87/1000, the losses are 1.29923 (train). The time used is 3.753s. \n",
      "At iter 88/1000, the losses are 1.29849 (train). The time used is 3.852s. \n",
      "At iter 89/1000, the losses are 1.29780 (train). The time used is 3.738s. \n",
      "At iter 90/1000, the losses are 1.29719 (train). The time used is 3.999s. \n",
      "At iter 91/1000, the losses are 1.29654 (train). The time used is 4.072s. \n",
      "At iter 92/1000, the losses are 1.29598 (train). The time used is 3.828s. \n",
      "At iter 93/1000, the losses are 1.29544 (train). The time used is 3.846s. \n",
      "At iter 94/1000, the losses are 1.29484 (train). The time used is 3.769s. \n",
      "At iter 95/1000, the losses are 1.29429 (train). The time used is 3.796s. \n",
      "At iter 96/1000, the losses are 1.29368 (train). The time used is 3.778s. \n",
      "At iter 97/1000, the losses are 1.29311 (train). The time used is 4.213s. \n",
      "At iter 98/1000, the losses are 1.29250 (train). The time used is 4.537s. \n",
      "At iter 99/1000, the losses are 1.29183 (train). The time used is 4.533s. \n",
      "At iter 100/1000, the losses are 1.29106 (train). The time used is 4.724s. \n",
      "====================================================================================================\n",
      "At iter 100/1000, the losses on all data are 0.83271. The time used is 1.858s. \n",
      "====================================================================================================\n",
      "At iter 101/1000, the losses are 1.29064 (train). The time used is 4.362s. \n",
      "At iter 102/1000, the losses are 1.29047 (train). The time used is 4.549s. \n",
      "At iter 103/1000, the losses are 1.28937 (train). The time used is 4.114s. \n",
      "At iter 104/1000, the losses are 1.28737 (train). The time used is 4.425s. \n",
      "At iter 105/1000, the losses are 1.28634 (train). The time used is 4.458s. \n",
      "At iter 106/1000, the losses are 1.28521 (train). The time used is 3.755s. \n",
      "At iter 107/1000, the losses are 1.28300 (train). The time used is 3.756s. \n",
      "At iter 108/1000, the losses are 1.28235 (train). The time used is 4.199s. \n",
      "At iter 109/1000, the losses are 1.28090 (train). The time used is 4.557s. \n",
      "At iter 110/1000, the losses are 1.27895 (train). The time used is 4.661s. \n",
      "At iter 111/1000, the losses are 1.27813 (train). The time used is 4.543s. \n",
      "At iter 112/1000, the losses are 1.27578 (train). The time used is 4.589s. \n",
      "At iter 113/1000, the losses are 1.27319 (train). The time used is 4.476s. \n",
      "At iter 114/1000, the losses are 1.26967 (train). The time used is 4.711s. \n",
      "At iter 115/1000, the losses are 1.26524 (train). The time used is 4.141s. \n",
      "At iter 116/1000, the losses are 1.26050 (train). The time used is 3.806s. \n",
      "At iter 117/1000, the losses are 1.25628 (train). The time used is 3.995s. \n",
      "At iter 118/1000, the losses are 1.25291 (train). The time used is 3.868s. \n",
      "At iter 119/1000, the losses are 1.24943 (train). The time used is 3.982s. \n",
      "At iter 120/1000, the losses are 1.24693 (train). The time used is 3.758s. \n",
      "====================================================================================================\n",
      "At iter 120/1000, the losses on all data are 0.83390. The time used is 1.411s. \n",
      "====================================================================================================\n",
      "At iter 121/1000, the losses are 1.24441 (train). The time used is 3.763s. \n",
      "At iter 122/1000, the losses are 1.24240 (train). The time used is 3.755s. \n",
      "At iter 123/1000, the losses are 1.24072 (train). The time used is 3.879s. \n",
      "At iter 124/1000, the losses are 1.23953 (train). The time used is 4.162s. \n",
      "At iter 125/1000, the losses are 1.23902 (train). The time used is 3.698s. \n",
      "At iter 126/1000, the losses are 1.23852 (train). The time used is 3.995s. \n",
      "At iter 127/1000, the losses are 1.23805 (train). The time used is 4.424s. \n",
      "At iter 128/1000, the losses are 1.23764 (train). The time used is 4.160s. \n",
      "At iter 129/1000, the losses are 1.23733 (train). The time used is 3.753s. \n",
      "At iter 130/1000, the losses are 1.23710 (train). The time used is 3.802s. \n",
      "At iter 131/1000, the losses are 1.23691 (train). The time used is 3.922s. \n",
      "At iter 132/1000, the losses are 1.23670 (train). The time used is 3.747s. \n",
      "At iter 133/1000, the losses are 1.23638 (train). The time used is 4.319s. \n",
      "At iter 134/1000, the losses are 1.23599 (train). The time used is 3.968s. \n",
      "At iter 135/1000, the losses are 1.23554 (train). The time used is 3.696s. \n",
      "At iter 136/1000, the losses are 1.23508 (train). The time used is 3.816s. \n",
      "At iter 137/1000, the losses are 1.23476 (train). The time used is 3.815s. \n",
      "At iter 138/1000, the losses are 1.23443 (train). The time used is 4.323s. \n",
      "At iter 139/1000, the losses are 1.23400 (train). The time used is 4.600s. \n",
      "At iter 140/1000, the losses are 1.23350 (train). The time used is 4.256s. \n",
      "====================================================================================================\n",
      "At iter 140/1000, the losses on all data are 0.83502. The time used is 1.430s. \n",
      "====================================================================================================\n",
      "At iter 141/1000, the losses are 1.23295 (train). The time used is 4.271s. \n",
      "At iter 142/1000, the losses are 1.23236 (train). The time used is 3.816s. \n",
      "At iter 143/1000, the losses are 1.23173 (train). The time used is 3.786s. \n",
      "At iter 144/1000, the losses are 1.23111 (train). The time used is 3.723s. \n",
      "At iter 145/1000, the losses are 1.23063 (train). The time used is 3.761s. \n",
      "At iter 146/1000, the losses are 1.23021 (train). The time used is 3.771s. \n",
      "At iter 147/1000, the losses are 1.22953 (train). The time used is 3.730s. \n",
      "At iter 148/1000, the losses are 1.22881 (train). The time used is 3.782s. \n",
      "At iter 149/1000, the losses are 1.22831 (train). The time used is 4.501s. \n",
      "At iter 150/1000, the losses are 1.22787 (train). The time used is 4.617s. \n",
      "At iter 151/1000, the losses are 1.22735 (train). The time used is 4.616s. \n",
      "At iter 152/1000, the losses are 1.22693 (train). The time used is 4.548s. \n",
      "At iter 153/1000, the losses are 1.22659 (train). The time used is 4.583s. \n",
      "At iter 154/1000, the losses are 1.22615 (train). The time used is 3.958s. \n",
      "At iter 155/1000, the losses are 1.22569 (train). The time used is 3.755s. \n",
      "At iter 156/1000, the losses are 1.22539 (train). The time used is 4.470s. \n",
      "At iter 157/1000, the losses are 1.22508 (train). The time used is 4.644s. \n",
      "At iter 158/1000, the losses are 1.22461 (train). The time used is 4.376s. \n",
      "At iter 159/1000, the losses are 1.22424 (train). The time used is 4.516s. \n",
      "At iter 160/1000, the losses are 1.22400 (train). The time used is 4.496s. \n",
      "====================================================================================================\n",
      "At iter 160/1000, the losses on all data are 0.83605. The time used is 1.728s. \n",
      "====================================================================================================\n",
      "At iter 161/1000, the losses are 1.22377 (train). The time used is 4.572s. \n",
      "At iter 162/1000, the losses are 1.22350 (train). The time used is 4.490s. \n",
      "At iter 163/1000, the losses are 1.22302 (train). The time used is 4.432s. \n",
      "At iter 164/1000, the losses are 1.22240 (train). The time used is 4.526s. \n",
      "At iter 165/1000, the losses are 1.22206 (train). The time used is 4.401s. \n",
      "At iter 166/1000, the losses are 1.22199 (train). The time used is 4.550s. \n",
      "At iter 167/1000, the losses are 1.22188 (train). The time used is 4.445s. \n",
      "At iter 168/1000, the losses are 1.22154 (train). The time used is 4.480s. \n",
      "At iter 169/1000, the losses are 1.22102 (train). The time used is 4.638s. \n",
      "At iter 170/1000, the losses are 1.22059 (train). The time used is 4.513s. \n",
      "At iter 171/1000, the losses are 1.22041 (train). The time used is 4.419s. \n",
      "At iter 172/1000, the losses are 1.22030 (train). The time used is 4.539s. \n",
      "At iter 173/1000, the losses are 1.21991 (train). The time used is 4.555s. \n",
      "At iter 174/1000, the losses are 1.21940 (train). The time used is 4.478s. \n",
      "At iter 175/1000, the losses are 1.21907 (train). The time used is 4.345s. \n",
      "At iter 176/1000, the losses are 1.21885 (train). The time used is 4.318s. \n",
      "At iter 177/1000, the losses are 1.21861 (train). The time used is 4.424s. \n",
      "At iter 178/1000, the losses are 1.21818 (train). The time used is 4.460s. \n",
      "At iter 179/1000, the losses are 1.21767 (train). The time used is 4.450s. \n",
      "At iter 180/1000, the losses are 1.21724 (train). The time used is 4.243s. \n",
      "====================================================================================================\n",
      "At iter 180/1000, the losses on all data are 0.83660. The time used is 1.585s. \n",
      "====================================================================================================\n",
      "At iter 181/1000, the losses are 1.21694 (train). The time used is 4.481s. \n",
      "At iter 182/1000, the losses are 1.21660 (train). The time used is 4.450s. \n",
      "At iter 183/1000, the losses are 1.21647 (train). The time used is 4.575s. \n",
      "At iter 184/1000, the losses are 1.21597 (train). The time used is 4.633s. \n",
      "At iter 185/1000, the losses are 1.21569 (train). The time used is 4.650s. \n",
      "At iter 186/1000, the losses are 1.21527 (train). The time used is 4.477s. \n",
      "At iter 187/1000, the losses are 1.21470 (train). The time used is 4.564s. \n",
      "At iter 188/1000, the losses are 1.21435 (train). The time used is 4.515s. \n",
      "At iter 189/1000, the losses are 1.21391 (train). The time used is 4.520s. \n",
      "At iter 190/1000, the losses are 1.21371 (train). The time used is 4.483s. \n",
      "At iter 191/1000, the losses are 1.21332 (train). The time used is 4.598s. \n",
      "At iter 192/1000, the losses are 1.21305 (train). The time used is 4.648s. \n",
      "At iter 193/1000, the losses are 1.21274 (train). The time used is 4.654s. \n",
      "At iter 194/1000, the losses are 1.21206 (train). The time used is 4.730s. \n",
      "At iter 195/1000, the losses are 1.21149 (train). The time used is 4.598s. \n",
      "At iter 196/1000, the losses are 1.21074 (train). The time used is 4.653s. \n",
      "At iter 197/1000, the losses are 1.21021 (train). The time used is 4.686s. \n",
      "At iter 198/1000, the losses are 1.20972 (train). The time used is 4.528s. \n",
      "At iter 199/1000, the losses are 1.20923 (train). The time used is 4.539s. \n",
      "At iter 200/1000, the losses are 1.20887 (train). The time used is 4.671s. \n",
      "====================================================================================================\n",
      "At iter 200/1000, the losses on all data are 0.83703. The time used is 1.765s. \n",
      "====================================================================================================\n",
      "At iter 201/1000, the losses are 1.20855 (train). The time used is 4.495s. \n",
      "At iter 202/1000, the losses are 1.20724 (train). The time used is 4.688s. \n",
      "At iter 203/1000, the losses are 1.20664 (train). The time used is 4.679s. \n",
      "At iter 204/1000, the losses are 1.20673 (train). The time used is 4.558s. \n",
      "At iter 205/1000, the losses are 1.20591 (train). The time used is 4.471s. \n",
      "At iter 206/1000, the losses are 1.20532 (train). The time used is 3.904s. \n",
      "At iter 207/1000, the losses are 1.20509 (train). The time used is 3.943s. \n",
      "At iter 208/1000, the losses are 1.20483 (train). The time used is 4.401s. \n",
      "At iter 209/1000, the losses are 1.20404 (train). The time used is 4.622s. \n",
      "At iter 210/1000, the losses are 1.20344 (train). The time used is 4.010s. \n",
      "At iter 211/1000, the losses are 1.20296 (train). The time used is 4.450s. \n",
      "At iter 212/1000, the losses are 1.20249 (train). The time used is 4.600s. \n",
      "At iter 213/1000, the losses are 1.20221 (train). The time used is 4.240s. \n",
      "At iter 214/1000, the losses are 1.20205 (train). The time used is 3.940s. \n",
      "At iter 215/1000, the losses are 1.20164 (train). The time used is 4.401s. \n",
      "At iter 216/1000, the losses are 1.20117 (train). The time used is 4.637s. \n",
      "At iter 217/1000, the losses are 1.20068 (train). The time used is 4.667s. \n",
      "At iter 218/1000, the losses are 1.20020 (train). The time used is 4.643s. \n",
      "At iter 219/1000, the losses are 1.19978 (train). The time used is 4.604s. \n",
      "At iter 220/1000, the losses are 1.19946 (train). The time used is 4.029s. \n",
      "====================================================================================================\n",
      "At iter 220/1000, the losses on all data are 0.83762. The time used is 1.418s. \n",
      "====================================================================================================\n",
      "At iter 221/1000, the losses are 1.19920 (train). The time used is 3.897s. \n",
      "At iter 222/1000, the losses are 1.19890 (train). The time used is 4.557s. \n",
      "At iter 223/1000, the losses are 1.19861 (train). The time used is 4.497s. \n",
      "At iter 224/1000, the losses are 1.19835 (train). The time used is 4.633s. \n",
      "At iter 225/1000, the losses are 1.19777 (train). The time used is 4.569s. \n",
      "At iter 226/1000, the losses are 1.19729 (train). The time used is 4.448s. \n",
      "At iter 227/1000, the losses are 1.19692 (train). The time used is 4.116s. \n",
      "At iter 228/1000, the losses are 1.19662 (train). The time used is 4.035s. \n",
      "At iter 229/1000, the losses are 1.19634 (train). The time used is 3.898s. \n",
      "At iter 230/1000, the losses are 1.19613 (train). The time used is 3.816s. \n",
      "At iter 231/1000, the losses are 1.19607 (train). The time used is 4.260s. \n",
      "At iter 232/1000, the losses are 1.19595 (train). The time used is 3.937s. \n",
      "At iter 233/1000, the losses are 1.19522 (train). The time used is 3.862s. \n",
      "At iter 234/1000, the losses are 1.19491 (train). The time used is 3.877s. \n",
      "At iter 235/1000, the losses are 1.19482 (train). The time used is 3.896s. \n",
      "At iter 236/1000, the losses are 1.19498 (train). The time used is 3.835s. \n",
      "At iter 237/1000, the losses are 1.19423 (train). The time used is 4.424s. \n",
      "At iter 238/1000, the losses are 1.19397 (train). The time used is 4.603s. \n",
      "At iter 239/1000, the losses are 1.19379 (train). The time used is 4.566s. \n",
      "At iter 240/1000, the losses are 1.19372 (train). The time used is 4.137s. \n",
      "====================================================================================================\n",
      "At iter 240/1000, the losses on all data are 0.83786. The time used is 1.420s. \n",
      "====================================================================================================\n",
      "At iter 241/1000, the losses are 1.19403 (train). The time used is 3.727s. \n",
      "At iter 242/1000, the losses are 1.19296 (train). The time used is 4.265s. \n",
      "At iter 243/1000, the losses are 1.19322 (train). The time used is 4.548s. \n",
      "At iter 244/1000, the losses are 1.19351 (train). The time used is 4.399s. \n",
      "At iter 245/1000, the losses are 1.19239 (train). The time used is 4.618s. \n",
      "At iter 246/1000, the losses are 1.19431 (train). The time used is 4.614s. \n",
      "At iter 247/1000, the losses are 1.19264 (train). The time used is 4.634s. \n",
      "At iter 248/1000, the losses are 1.19559 (train). The time used is 4.189s. \n",
      "At iter 249/1000, the losses are 1.19683 (train). The time used is 3.842s. \n",
      "At iter 250/1000, the losses are 1.19173 (train). The time used is 3.783s. \n",
      "At iter 251/1000, the losses are 1.19938 (train). The time used is 3.835s. \n",
      "At iter 252/1000, the losses are 1.20461 (train). The time used is 3.838s. \n",
      "At iter 253/1000, the losses are 1.19718 (train). The time used is 3.788s. \n",
      "At iter 254/1000, the losses are 1.19226 (train). The time used is 3.738s. \n",
      "At iter 255/1000, the losses are 1.19549 (train). The time used is 4.217s. \n",
      "At iter 256/1000, the losses are 1.19370 (train). The time used is 4.656s. \n",
      "At iter 257/1000, the losses are 1.19372 (train). The time used is 4.702s. \n",
      "At iter 258/1000, the losses are 1.19741 (train). The time used is 4.198s. \n",
      "At iter 259/1000, the losses are 1.19432 (train). The time used is 3.752s. \n",
      "At iter 260/1000, the losses are 1.19345 (train). The time used is 3.745s. \n",
      "====================================================================================================\n",
      "At iter 260/1000, the losses on all data are 0.83722. The time used is 1.418s. \n",
      "====================================================================================================\n",
      "At iter 261/1000, the losses are 1.19935 (train). The time used is 3.703s. \n",
      "At iter 262/1000, the losses are 1.19545 (train). The time used is 3.755s. \n",
      "At iter 263/1000, the losses are 1.19183 (train). The time used is 3.769s. \n",
      "At iter 264/1000, the losses are 1.19613 (train). The time used is 3.833s. \n",
      "At iter 265/1000, the losses are 1.19351 (train). The time used is 3.803s. \n",
      "At iter 266/1000, the losses are 1.19211 (train). The time used is 3.813s. \n",
      "At iter 267/1000, the losses are 1.19587 (train). The time used is 3.794s. \n",
      "At iter 268/1000, the losses are 1.19364 (train). The time used is 3.818s. \n",
      "At iter 269/1000, the losses are 1.19066 (train). The time used is 3.866s. \n",
      "At iter 270/1000, the losses are 1.19272 (train). The time used is 4.037s. \n",
      "At iter 271/1000, the losses are 1.19345 (train). The time used is 3.811s. \n",
      "At iter 272/1000, the losses are 1.18971 (train). The time used is 3.755s. \n",
      "At iter 273/1000, the losses are 1.19093 (train). The time used is 3.756s. \n",
      "At iter 274/1000, the losses are 1.19208 (train). The time used is 3.735s. \n",
      "At iter 275/1000, the losses are 1.18903 (train). The time used is 3.807s. \n",
      "At iter 276/1000, the losses are 1.19082 (train). The time used is 4.395s. \n",
      "At iter 277/1000, the losses are 1.19023 (train). The time used is 4.597s. \n",
      "At iter 278/1000, the losses are 1.18910 (train). The time used is 4.056s. \n",
      "At iter 279/1000, the losses are 1.19114 (train). The time used is 4.348s. \n",
      "At iter 280/1000, the losses are 1.18916 (train). The time used is 4.549s. \n",
      "====================================================================================================\n",
      "At iter 280/1000, the losses on all data are 0.83847. The time used is 1.791s. \n",
      "====================================================================================================\n",
      "At iter 281/1000, the losses are 1.18831 (train). The time used is 4.519s. \n",
      "At iter 282/1000, the losses are 1.18965 (train). The time used is 3.769s. \n",
      "At iter 283/1000, the losses are 1.18798 (train). The time used is 3.781s. \n",
      "At iter 284/1000, the losses are 1.18975 (train). The time used is 3.849s. \n",
      "At iter 285/1000, the losses are 1.19146 (train). The time used is 4.482s. \n",
      "At iter 286/1000, the losses are 1.18915 (train). The time used is 4.400s. \n",
      "At iter 287/1000, the losses are 1.18880 (train). The time used is 3.783s. \n",
      "At iter 288/1000, the losses are 1.19192 (train). The time used is 3.788s. \n",
      "At iter 289/1000, the losses are 1.19064 (train). The time used is 3.849s. \n",
      "At iter 290/1000, the losses are 1.18732 (train). The time used is 4.380s. \n",
      "At iter 291/1000, the losses are 1.19081 (train). The time used is 4.526s. \n",
      "At iter 292/1000, the losses are 1.19224 (train). The time used is 4.562s. \n",
      "At iter 293/1000, the losses are 1.18834 (train). The time used is 4.593s. \n",
      "At iter 294/1000, the losses are 1.18838 (train). The time used is 4.594s. \n",
      "At iter 295/1000, the losses are 1.19120 (train). The time used is 4.550s. \n",
      "At iter 296/1000, the losses are 1.18935 (train). The time used is 4.536s. \n",
      "At iter 297/1000, the losses are 1.18651 (train). The time used is 4.500s. \n",
      "At iter 298/1000, the losses are 1.18930 (train). The time used is 4.120s. \n",
      "At iter 299/1000, the losses are 1.19001 (train). The time used is 3.921s. \n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "At iter 300/1000, the losses are 1.18732 (train). The time used is 3.718s. \n",
      "====================================================================================================\n",
      "At iter 300/1000, the losses on all data are 0.83860. The time used is 1.415s. \n",
      "====================================================================================================\n",
      "At iter 301/1000, the losses are 1.18695 (train). The time used is 3.789s. \n",
      "At iter 302/1000, the losses are 1.18745 (train). The time used is 3.775s. \n",
      "At iter 303/1000, the losses are 1.18637 (train). The time used is 3.952s. \n",
      "At iter 304/1000, the losses are 1.18582 (train). The time used is 4.575s. \n",
      "At iter 305/1000, the losses are 1.18666 (train). The time used is 4.597s. \n",
      "At iter 306/1000, the losses are 1.18655 (train). The time used is 4.500s. \n",
      "At iter 307/1000, the losses are 1.18578 (train). The time used is 4.610s. \n",
      "At iter 308/1000, the losses are 1.18573 (train). The time used is 4.650s. \n",
      "At iter 309/1000, the losses are 1.18620 (train). The time used is 3.956s. \n",
      "At iter 310/1000, the losses are 1.18577 (train). The time used is 3.780s. \n",
      "At iter 311/1000, the losses are 1.18536 (train). The time used is 3.803s. \n",
      "At iter 312/1000, the losses are 1.18568 (train). The time used is 3.843s. \n",
      "At iter 313/1000, the losses are 1.18567 (train). The time used is 4.065s. \n",
      "At iter 314/1000, the losses are 1.18525 (train). The time used is 3.987s. \n",
      "At iter 315/1000, the losses are 1.18517 (train). The time used is 3.762s. \n",
      "At iter 316/1000, the losses are 1.18541 (train). The time used is 4.347s. \n",
      "At iter 317/1000, the losses are 1.18512 (train). The time used is 4.493s. \n",
      "At iter 318/1000, the losses are 1.18491 (train). The time used is 4.464s. \n",
      "At iter 319/1000, the losses are 1.18503 (train). The time used is 4.145s. \n",
      "At iter 320/1000, the losses are 1.18497 (train). The time used is 3.901s. \n",
      "====================================================================================================\n",
      "At iter 320/1000, the losses on all data are 0.83861. The time used is 1.415s. \n",
      "====================================================================================================\n",
      "At iter 321/1000, the losses are 1.18470 (train). The time used is 4.116s. \n",
      "At iter 322/1000, the losses are 1.18470 (train). The time used is 4.526s. \n",
      "At iter 323/1000, the losses are 1.18474 (train). The time used is 4.079s. \n",
      "At iter 324/1000, the losses are 1.18451 (train). The time used is 3.875s. \n",
      "At iter 325/1000, the losses are 1.18444 (train). The time used is 4.328s. \n",
      "At iter 326/1000, the losses are 1.18447 (train). The time used is 3.799s. \n",
      "At iter 327/1000, the losses are 1.18433 (train). The time used is 3.858s. \n",
      "At iter 328/1000, the losses are 1.18421 (train). The time used is 4.084s. \n",
      "At iter 329/1000, the losses are 1.18425 (train). The time used is 4.539s. \n",
      "At iter 330/1000, the losses are 1.18411 (train). The time used is 4.030s. \n",
      "At iter 331/1000, the losses are 1.18401 (train). The time used is 3.900s. \n",
      "At iter 332/1000, the losses are 1.18402 (train). The time used is 3.855s. \n",
      "At iter 333/1000, the losses are 1.18392 (train). The time used is 3.867s. \n",
      "At iter 334/1000, the losses are 1.18381 (train). The time used is 3.804s. \n",
      "At iter 335/1000, the losses are 1.18381 (train). The time used is 3.831s. \n",
      "At iter 336/1000, the losses are 1.18371 (train). The time used is 3.800s. \n",
      "At iter 337/1000, the losses are 1.18362 (train). The time used is 3.902s. \n",
      "At iter 338/1000, the losses are 1.18360 (train). The time used is 3.811s. \n",
      "At iter 339/1000, the losses are 1.18351 (train). The time used is 3.877s. \n",
      "At iter 340/1000, the losses are 1.18343 (train). The time used is 4.274s. \n",
      "====================================================================================================\n",
      "At iter 340/1000, the losses on all data are 0.83876. The time used is 1.421s. \n",
      "====================================================================================================\n",
      "At iter 341/1000, the losses are 1.18340 (train). The time used is 3.808s. \n",
      "At iter 342/1000, the losses are 1.18331 (train). The time used is 3.874s. \n",
      "At iter 343/1000, the losses are 1.18325 (train). The time used is 3.934s. \n",
      "At iter 344/1000, the losses are 1.18321 (train). The time used is 3.763s. \n",
      "At iter 345/1000, the losses are 1.18311 (train). The time used is 3.785s. \n",
      "At iter 346/1000, the losses are 1.18306 (train). The time used is 3.983s. \n",
      "At iter 347/1000, the losses are 1.18300 (train). The time used is 3.826s. \n",
      "At iter 348/1000, the losses are 1.18292 (train). The time used is 3.936s. \n",
      "At iter 349/1000, the losses are 1.18287 (train). The time used is 3.843s. \n",
      "At iter 350/1000, the losses are 1.18280 (train). The time used is 3.800s. \n",
      "At iter 351/1000, the losses are 1.18273 (train). The time used is 4.258s. \n",
      "At iter 352/1000, the losses are 1.18268 (train). The time used is 4.517s. \n",
      "At iter 353/1000, the losses are 1.18260 (train). The time used is 4.532s. \n",
      "At iter 354/1000, the losses are 1.18254 (train). The time used is 4.454s. \n",
      "At iter 355/1000, the losses are 1.18247 (train). The time used is 4.482s. \n",
      "At iter 356/1000, the losses are 1.18240 (train). The time used is 3.889s. \n",
      "At iter 357/1000, the losses are 1.18234 (train). The time used is 4.410s. \n",
      "At iter 358/1000, the losses are 1.18227 (train). The time used is 4.365s. \n",
      "At iter 359/1000, the losses are 1.18220 (train). The time used is 4.206s. \n",
      "At iter 360/1000, the losses are 1.18213 (train). The time used is 3.781s. \n",
      "====================================================================================================\n",
      "At iter 360/1000, the losses on all data are 0.83884. The time used is 1.420s. \n",
      "====================================================================================================\n",
      "At iter 361/1000, the losses are 1.18207 (train). The time used is 3.932s. \n",
      "At iter 362/1000, the losses are 1.18200 (train). The time used is 3.813s. \n",
      "At iter 363/1000, the losses are 1.18194 (train). The time used is 4.254s. \n",
      "At iter 364/1000, the losses are 1.18188 (train). The time used is 4.565s. \n",
      "At iter 365/1000, the losses are 1.18181 (train). The time used is 4.553s. \n",
      "At iter 366/1000, the losses are 1.18175 (train). The time used is 4.692s. \n",
      "At iter 367/1000, the losses are 1.18168 (train). The time used is 4.550s. \n",
      "At iter 368/1000, the losses are 1.18162 (train). The time used is 4.196s. \n",
      "At iter 369/1000, the losses are 1.18156 (train). The time used is 3.838s. \n",
      "At iter 370/1000, the losses are 1.18150 (train). The time used is 3.820s. \n",
      "At iter 371/1000, the losses are 1.18144 (train). The time used is 3.843s. \n",
      "At iter 372/1000, the losses are 1.18138 (train). The time used is 3.904s. \n",
      "At iter 373/1000, the losses are 1.18132 (train). The time used is 3.859s. \n",
      "At iter 374/1000, the losses are 1.18126 (train). The time used is 3.974s. \n",
      "At iter 375/1000, the losses are 1.18119 (train). The time used is 4.259s. \n",
      "At iter 376/1000, the losses are 1.18113 (train). The time used is 3.801s. \n",
      "At iter 377/1000, the losses are 1.18107 (train). The time used is 3.962s. \n",
      "At iter 378/1000, the losses are 1.18101 (train). The time used is 4.333s. \n",
      "At iter 379/1000, the losses are 1.18095 (train). The time used is 4.484s. \n",
      "At iter 380/1000, the losses are 1.18089 (train). The time used is 4.465s. \n",
      "====================================================================================================\n",
      "At iter 380/1000, the losses on all data are 0.83891. The time used is 1.419s. \n",
      "====================================================================================================\n",
      "At iter 381/1000, the losses are 1.18082 (train). The time used is 4.565s. \n",
      "At iter 382/1000, the losses are 1.18076 (train). The time used is 4.259s. \n",
      "At iter 383/1000, the losses are 1.18069 (train). The time used is 4.607s. \n",
      "At iter 384/1000, the losses are 1.18063 (train). The time used is 4.616s. \n",
      "At iter 385/1000, the losses are 1.18057 (train). The time used is 4.529s. \n",
      "At iter 386/1000, the losses are 1.18050 (train). The time used is 4.374s. \n",
      "At iter 387/1000, the losses are 1.18044 (train). The time used is 3.884s. \n",
      "At iter 388/1000, the losses are 1.18037 (train). The time used is 3.808s. \n",
      "At iter 389/1000, the losses are 1.18031 (train). The time used is 4.241s. \n",
      "At iter 390/1000, the losses are 1.18024 (train). The time used is 3.941s. \n",
      "At iter 391/1000, the losses are 1.18018 (train). The time used is 3.862s. \n",
      "At iter 392/1000, the losses are 1.18011 (train). The time used is 3.771s. \n",
      "At iter 393/1000, the losses are 1.18005 (train). The time used is 3.782s. \n",
      "At iter 394/1000, the losses are 1.17998 (train). The time used is 3.811s. \n",
      "At iter 395/1000, the losses are 1.17992 (train). The time used is 3.779s. \n",
      "At iter 396/1000, the losses are 1.17986 (train). The time used is 3.859s. \n",
      "At iter 397/1000, the losses are 1.17979 (train). The time used is 3.973s. \n",
      "At iter 398/1000, the losses are 1.17973 (train). The time used is 3.855s. \n",
      "At iter 399/1000, the losses are 1.17967 (train). The time used is 3.998s. \n",
      "At iter 400/1000, the losses are 1.17961 (train). The time used is 3.878s. \n",
      "====================================================================================================\n",
      "At iter 400/1000, the losses on all data are 0.83899. The time used is 1.442s. \n",
      "====================================================================================================\n",
      "At iter 401/1000, the losses are 1.17955 (train). The time used is 4.020s. \n",
      "At iter 402/1000, the losses are 1.17949 (train). The time used is 3.919s. \n",
      "At iter 403/1000, the losses are 1.17943 (train). The time used is 3.847s. \n",
      "At iter 404/1000, the losses are 1.17937 (train). The time used is 4.298s. \n",
      "At iter 405/1000, the losses are 1.17931 (train). The time used is 4.652s. \n",
      "At iter 406/1000, the losses are 1.17926 (train). The time used is 4.649s. \n",
      "At iter 407/1000, the losses are 1.17920 (train). The time used is 4.014s. \n",
      "At iter 408/1000, the losses are 1.17914 (train). The time used is 3.756s. \n",
      "At iter 409/1000, the losses are 1.17907 (train). The time used is 3.753s. \n",
      "At iter 410/1000, the losses are 1.17900 (train). The time used is 3.901s. \n",
      "At iter 411/1000, the losses are 1.17894 (train). The time used is 3.839s. \n",
      "At iter 412/1000, the losses are 1.17889 (train). The time used is 4.371s. \n",
      "At iter 413/1000, the losses are 1.17883 (train). The time used is 4.586s. \n",
      "At iter 414/1000, the losses are 1.17878 (train). The time used is 4.193s. \n",
      "At iter 415/1000, the losses are 1.17873 (train). The time used is 3.865s. \n",
      "At iter 416/1000, the losses are 1.17868 (train). The time used is 3.916s. \n",
      "At iter 417/1000, the losses are 1.17863 (train). The time used is 3.883s. \n",
      "At iter 418/1000, the losses are 1.17857 (train). The time used is 3.881s. \n",
      "At iter 419/1000, the losses are 1.17851 (train). The time used is 3.883s. \n",
      "At iter 420/1000, the losses are 1.17843 (train). The time used is 4.594s. \n",
      "====================================================================================================\n",
      "At iter 420/1000, the losses on all data are 0.83907. The time used is 1.818s. \n",
      "====================================================================================================\n",
      "At iter 421/1000, the losses are 1.17837 (train). The time used is 4.641s. \n",
      "At iter 422/1000, the losses are 1.17832 (train). The time used is 4.278s. \n",
      "At iter 423/1000, the losses are 1.17827 (train). The time used is 3.842s. \n",
      "At iter 424/1000, the losses are 1.17822 (train). The time used is 4.245s. \n",
      "At iter 425/1000, the losses are 1.17817 (train). The time used is 4.452s. \n",
      "At iter 426/1000, the losses are 1.17812 (train). The time used is 4.368s. \n",
      "At iter 427/1000, the losses are 1.17809 (train). The time used is 4.336s. \n",
      "At iter 428/1000, the losses are 1.17805 (train). The time used is 4.401s. \n",
      "At iter 429/1000, the losses are 1.17802 (train). The time used is 4.576s. \n",
      "At iter 430/1000, the losses are 1.17796 (train). The time used is 4.632s. \n",
      "At iter 431/1000, the losses are 1.17789 (train). The time used is 4.562s. \n",
      "At iter 432/1000, the losses are 1.17781 (train). The time used is 4.656s. \n",
      "At iter 433/1000, the losses are 1.17774 (train). The time used is 4.573s. \n",
      "At iter 434/1000, the losses are 1.17769 (train). The time used is 4.502s. \n",
      "At iter 435/1000, the losses are 1.17766 (train). The time used is 4.217s. \n",
      "At iter 436/1000, the losses are 1.17763 (train). The time used is 3.809s. \n",
      "At iter 437/1000, the losses are 1.17759 (train). The time used is 3.850s. \n",
      "At iter 438/1000, the losses are 1.17754 (train). The time used is 3.846s. \n",
      "At iter 439/1000, the losses are 1.17750 (train). The time used is 4.038s. \n",
      "At iter 440/1000, the losses are 1.17747 (train). The time used is 4.512s. \n",
      "====================================================================================================\n",
      "At iter 440/1000, the losses on all data are 0.83908. The time used is 1.840s. \n",
      "====================================================================================================\n",
      "At iter 441/1000, the losses are 1.17742 (train). The time used is 4.511s. \n",
      "At iter 442/1000, the losses are 1.17734 (train). The time used is 4.653s. \n",
      "At iter 443/1000, the losses are 1.17725 (train). The time used is 4.560s. \n",
      "At iter 444/1000, the losses are 1.17722 (train). The time used is 4.029s. \n",
      "At iter 445/1000, the losses are 1.17721 (train). The time used is 3.858s. \n",
      "At iter 446/1000, the losses are 1.17715 (train). The time used is 4.351s. \n",
      "At iter 447/1000, the losses are 1.17709 (train). The time used is 4.518s. \n",
      "At iter 448/1000, the losses are 1.17707 (train). The time used is 4.078s. \n",
      "At iter 449/1000, the losses are 1.17705 (train). The time used is 3.916s. \n",
      "At iter 450/1000, the losses are 1.17699 (train). The time used is 4.657s. \n",
      "At iter 451/1000, the losses are 1.17694 (train). The time used is 4.669s. \n",
      "At iter 452/1000, the losses are 1.17691 (train). The time used is 4.565s. \n",
      "At iter 453/1000, the losses are 1.17687 (train). The time used is 4.634s. \n",
      "At iter 454/1000, the losses are 1.17681 (train). The time used is 4.434s. \n",
      "At iter 455/1000, the losses are 1.17674 (train). The time used is 3.965s. \n",
      "At iter 456/1000, the losses are 1.17670 (train). The time used is 4.543s. \n",
      "At iter 457/1000, the losses are 1.17666 (train). The time used is 4.722s. \n",
      "At iter 458/1000, the losses are 1.17659 (train). The time used is 4.611s. \n",
      "At iter 459/1000, the losses are 1.17654 (train). The time used is 4.315s. \n",
      "At iter 460/1000, the losses are 1.17650 (train). The time used is 4.461s. \n",
      "====================================================================================================\n",
      "At iter 460/1000, the losses on all data are 0.83918. The time used is 1.760s. \n",
      "====================================================================================================\n",
      "At iter 461/1000, the losses are 1.17646 (train). The time used is 4.634s. \n",
      "At iter 462/1000, the losses are 1.17641 (train). The time used is 4.705s. \n",
      "At iter 463/1000, the losses are 1.17637 (train). The time used is 4.591s. \n",
      "At iter 464/1000, the losses are 1.17634 (train). The time used is 4.570s. \n",
      "At iter 465/1000, the losses are 1.17632 (train). The time used is 4.732s. \n",
      "At iter 466/1000, the losses are 1.17635 (train). The time used is 4.589s. \n",
      "At iter 467/1000, the losses are 1.17644 (train). The time used is 4.516s. \n",
      "At iter 468/1000, the losses are 1.17663 (train). The time used is 4.531s. \n",
      "At iter 469/1000, the losses are 1.17622 (train). The time used is 4.445s. \n",
      "At iter 470/1000, the losses are 1.17611 (train). The time used is 3.756s. \n",
      "At iter 471/1000, the losses are 1.17619 (train). The time used is 3.935s. \n",
      "At iter 472/1000, the losses are 1.17635 (train). The time used is 3.905s. \n",
      "At iter 473/1000, the losses are 1.17625 (train). The time used is 4.321s. \n",
      "At iter 474/1000, the losses are 1.17611 (train). The time used is 4.299s. \n",
      "At iter 475/1000, the losses are 1.17596 (train). The time used is 3.913s. \n",
      "At iter 476/1000, the losses are 1.17587 (train). The time used is 4.356s. \n",
      "At iter 477/1000, the losses are 1.17591 (train). The time used is 4.612s. \n",
      "At iter 478/1000, the losses are 1.17597 (train). The time used is 4.603s. \n",
      "At iter 479/1000, the losses are 1.17591 (train). The time used is 4.454s. \n",
      "At iter 480/1000, the losses are 1.17579 (train). The time used is 4.564s. \n",
      "====================================================================================================\n",
      "At iter 480/1000, the losses on all data are 0.83919. The time used is 1.420s. \n",
      "====================================================================================================\n",
      "At iter 481/1000, the losses are 1.17570 (train). The time used is 3.927s. \n",
      "At iter 482/1000, the losses are 1.17568 (train). The time used is 3.811s. \n",
      "At iter 483/1000, the losses are 1.17566 (train). The time used is 3.908s. \n",
      "At iter 484/1000, the losses are 1.17564 (train). The time used is 3.833s. \n",
      "At iter 485/1000, the losses are 1.17562 (train). The time used is 3.769s. \n",
      "At iter 486/1000, the losses are 1.17559 (train). The time used is 3.750s. \n",
      "At iter 487/1000, the losses are 1.17551 (train). The time used is 3.737s. \n",
      "At iter 488/1000, the losses are 1.17543 (train). The time used is 3.794s. \n",
      "At iter 489/1000, the losses are 1.17540 (train). The time used is 3.765s. \n",
      "At iter 490/1000, the losses are 1.17538 (train). The time used is 3.831s. \n",
      "At iter 491/1000, the losses are 1.17537 (train). The time used is 4.700s. \n",
      "At iter 492/1000, the losses are 1.17533 (train). The time used is 3.920s. \n",
      "At iter 493/1000, the losses are 1.17531 (train). The time used is 3.746s. \n",
      "At iter 494/1000, the losses are 1.17529 (train). The time used is 4.238s. \n",
      "At iter 495/1000, the losses are 1.17527 (train). The time used is 4.583s. \n",
      "At iter 496/1000, the losses are 1.17523 (train). The time used is 4.463s. \n",
      "At iter 497/1000, the losses are 1.17518 (train). The time used is 3.884s. \n",
      "At iter 498/1000, the losses are 1.17514 (train). The time used is 3.855s. \n",
      "At iter 499/1000, the losses are 1.17510 (train). The time used is 3.847s. \n",
      "At iter 500/1000, the losses are 1.17506 (train). The time used is 3.881s. \n",
      "====================================================================================================\n",
      "At iter 500/1000, the losses on all data are 0.83923. The time used is 1.426s. \n",
      "====================================================================================================\n",
      "At iter 501/1000, the losses are 1.17501 (train). The time used is 4.240s. \n",
      "At iter 502/1000, the losses are 1.17496 (train). The time used is 4.664s. \n",
      "At iter 503/1000, the losses are 1.17491 (train). The time used is 4.543s. \n",
      "At iter 504/1000, the losses are 1.17487 (train). The time used is 4.665s. \n",
      "At iter 505/1000, the losses are 1.17484 (train). The time used is 4.585s. \n",
      "At iter 506/1000, the losses are 1.17481 (train). The time used is 3.987s. \n",
      "At iter 507/1000, the losses are 1.17477 (train). The time used is 3.758s. \n",
      "At iter 508/1000, the losses are 1.17473 (train). The time used is 3.817s. \n",
      "At iter 509/1000, the losses are 1.17469 (train). The time used is 4.246s. \n",
      "At iter 510/1000, the losses are 1.17466 (train). The time used is 4.344s. \n",
      "At iter 511/1000, the losses are 1.17462 (train). The time used is 4.479s. \n",
      "At iter 512/1000, the losses are 1.17459 (train). The time used is 4.406s. \n",
      "At iter 513/1000, the losses are 1.17456 (train). The time used is 4.511s. \n",
      "At iter 514/1000, the losses are 1.17453 (train). The time used is 4.420s. \n",
      "At iter 515/1000, the losses are 1.17451 (train). The time used is 4.452s. \n",
      "At iter 516/1000, the losses are 1.17451 (train). The time used is 4.416s. \n",
      "At iter 517/1000, the losses are 1.17454 (train). The time used is 4.386s. \n",
      "At iter 518/1000, the losses are 1.17464 (train). The time used is 4.574s. \n",
      "At iter 519/1000, the losses are 1.17490 (train). The time used is 4.529s. \n",
      "At iter 520/1000, the losses are 1.17440 (train). The time used is 4.517s. \n",
      "====================================================================================================\n",
      "At iter 520/1000, the losses on all data are 0.83931. The time used is 1.802s. \n",
      "====================================================================================================\n",
      "At iter 521/1000, the losses are 1.17443 (train). The time used is 4.526s. \n",
      "At iter 522/1000, the losses are 1.17468 (train). The time used is 4.534s. \n",
      "At iter 523/1000, the losses are 1.17480 (train). The time used is 4.536s. \n",
      "At iter 524/1000, the losses are 1.17432 (train). The time used is 4.374s. \n",
      "At iter 525/1000, the losses are 1.17470 (train). The time used is 4.642s. \n",
      "At iter 526/1000, the losses are 1.17478 (train). The time used is 4.455s. \n",
      "At iter 527/1000, the losses are 1.17434 (train). The time used is 4.438s. \n",
      "At iter 528/1000, the losses are 1.17484 (train). The time used is 4.508s. \n",
      "At iter 529/1000, the losses are 1.17429 (train). The time used is 4.011s. \n",
      "At iter 530/1000, the losses are 1.17431 (train). The time used is 3.767s. \n",
      "At iter 531/1000, the losses are 1.17473 (train). The time used is 3.816s. \n",
      "At iter 532/1000, the losses are 1.17415 (train). The time used is 3.787s. \n",
      "At iter 533/1000, the losses are 1.17426 (train). The time used is 3.939s. \n",
      "At iter 534/1000, the losses are 1.17430 (train). The time used is 4.517s. \n",
      "At iter 535/1000, the losses are 1.17392 (train). The time used is 4.528s. \n",
      "At iter 536/1000, the losses are 1.17428 (train). The time used is 3.744s. \n",
      "At iter 537/1000, the losses are 1.17417 (train). The time used is 4.248s. \n",
      "At iter 538/1000, the losses are 1.17394 (train). The time used is 4.551s. \n",
      "At iter 539/1000, the losses are 1.17414 (train). The time used is 4.192s. \n",
      "At iter 540/1000, the losses are 1.17384 (train). The time used is 4.653s. \n",
      "====================================================================================================\n",
      "At iter 540/1000, the losses on all data are 0.83928. The time used is 1.832s. \n",
      "====================================================================================================\n",
      "At iter 541/1000, the losses are 1.17384 (train). The time used is 4.744s. \n",
      "At iter 542/1000, the losses are 1.17397 (train). The time used is 4.615s. \n",
      "At iter 543/1000, the losses are 1.17374 (train). The time used is 4.687s. \n",
      "At iter 544/1000, the losses are 1.17380 (train). The time used is 4.290s. \n",
      "At iter 545/1000, the losses are 1.17376 (train). The time used is 4.539s. \n",
      "At iter 546/1000, the losses are 1.17360 (train). The time used is 4.097s. \n",
      "At iter 547/1000, the losses are 1.17369 (train). The time used is 3.789s. \n",
      "At iter 548/1000, the losses are 1.17361 (train). The time used is 3.820s. \n",
      "At iter 549/1000, the losses are 1.17356 (train). The time used is 3.994s. \n",
      "At iter 550/1000, the losses are 1.17361 (train). The time used is 4.459s. \n",
      "At iter 551/1000, the losses are 1.17350 (train). The time used is 4.655s. \n",
      "At iter 552/1000, the losses are 1.17348 (train). The time used is 4.561s. \n",
      "At iter 553/1000, the losses are 1.17349 (train). The time used is 4.515s. \n",
      "At iter 554/1000, the losses are 1.17340 (train). The time used is 4.520s. \n",
      "At iter 555/1000, the losses are 1.17340 (train). The time used is 4.114s. \n",
      "At iter 556/1000, the losses are 1.17341 (train). The time used is 3.846s. \n",
      "At iter 557/1000, the losses are 1.17334 (train). The time used is 4.133s. \n",
      "At iter 558/1000, the losses are 1.17333 (train). The time used is 4.017s. \n",
      "At iter 559/1000, the losses are 1.17333 (train). The time used is 3.831s. \n",
      "At iter 560/1000, the losses are 1.17326 (train). The time used is 4.640s. \n",
      "====================================================================================================\n",
      "At iter 560/1000, the losses on all data are 0.83936. The time used is 1.432s. \n",
      "====================================================================================================\n",
      "At iter 561/1000, the losses are 1.17325 (train). The time used is 3.775s. \n",
      "At iter 562/1000, the losses are 1.17325 (train). The time used is 3.840s. \n",
      "At iter 563/1000, the losses are 1.17321 (train). The time used is 3.819s. \n",
      "At iter 564/1000, the losses are 1.17319 (train). The time used is 3.827s. \n",
      "At iter 565/1000, the losses are 1.17321 (train). The time used is 3.808s. \n",
      "At iter 566/1000, the losses are 1.17319 (train). The time used is 3.755s. \n",
      "At iter 567/1000, the losses are 1.17317 (train). The time used is 3.930s. \n",
      "At iter 568/1000, the losses are 1.17316 (train). The time used is 3.834s. \n",
      "At iter 569/1000, the losses are 1.17313 (train). The time used is 3.879s. \n",
      "At iter 570/1000, the losses are 1.17308 (train). The time used is 3.747s. \n",
      "At iter 571/1000, the losses are 1.17303 (train). The time used is 4.454s. \n",
      "At iter 572/1000, the losses are 1.17298 (train). The time used is 3.872s. \n",
      "At iter 573/1000, the losses are 1.17293 (train). The time used is 3.863s. \n",
      "At iter 574/1000, the losses are 1.17289 (train). The time used is 3.929s. \n",
      "At iter 575/1000, the losses are 1.17286 (train). The time used is 4.570s. \n",
      "At iter 576/1000, the losses are 1.17284 (train). The time used is 4.574s. \n",
      "At iter 577/1000, the losses are 1.17282 (train). The time used is 4.614s. \n",
      "At iter 578/1000, the losses are 1.17280 (train). The time used is 4.523s. \n",
      "At iter 579/1000, the losses are 1.17279 (train). The time used is 4.570s. \n",
      "At iter 580/1000, the losses are 1.17280 (train). The time used is 4.517s. \n",
      "====================================================================================================\n",
      "At iter 580/1000, the losses on all data are 0.83941. The time used is 1.669s. \n",
      "====================================================================================================\n",
      "At iter 581/1000, the losses are 1.17283 (train). The time used is 4.407s. \n",
      "At iter 582/1000, the losses are 1.17289 (train). The time used is 4.522s. \n",
      "At iter 583/1000, the losses are 1.17296 (train). The time used is 4.531s. \n",
      "At iter 584/1000, the losses are 1.17290 (train). The time used is 4.313s. \n",
      "At iter 585/1000, the losses are 1.17280 (train). The time used is 3.761s. \n",
      "At iter 586/1000, the losses are 1.17268 (train). The time used is 3.805s. \n",
      "At iter 587/1000, the losses are 1.17259 (train). The time used is 3.849s. \n",
      "At iter 588/1000, the losses are 1.17254 (train). The time used is 3.872s. \n",
      "At iter 589/1000, the losses are 1.17252 (train). The time used is 3.871s. \n",
      "At iter 590/1000, the losses are 1.17253 (train). The time used is 3.787s. \n",
      "At iter 591/1000, the losses are 1.17256 (train). The time used is 3.804s. \n",
      "At iter 592/1000, the losses are 1.17258 (train). The time used is 3.769s. \n",
      "At iter 593/1000, the losses are 1.17258 (train). The time used is 3.853s. \n",
      "At iter 594/1000, the losses are 1.17254 (train). The time used is 3.962s. \n",
      "At iter 595/1000, the losses are 1.17246 (train). The time used is 3.763s. \n",
      "At iter 596/1000, the losses are 1.17238 (train). The time used is 3.884s. \n",
      "At iter 597/1000, the losses are 1.17232 (train). The time used is 3.798s. \n",
      "At iter 598/1000, the losses are 1.17229 (train). The time used is 4.187s. \n",
      "At iter 599/1000, the losses are 1.17227 (train). The time used is 3.928s. \n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "At iter 600/1000, the losses are 1.17227 (train). The time used is 4.168s. \n",
      "====================================================================================================\n",
      "At iter 600/1000, the losses on all data are 0.83944. The time used is 1.427s. \n",
      "====================================================================================================\n",
      "At iter 601/1000, the losses are 1.17228 (train). The time used is 4.642s. \n",
      "At iter 602/1000, the losses are 1.17220 (train). The time used is 4.595s. \n",
      "At iter 603/1000, the losses are 1.17225 (train). The time used is 4.472s. \n",
      "At iter 604/1000, the losses are 1.17217 (train). The time used is 3.977s. \n",
      "At iter 605/1000, the losses are 1.17221 (train). The time used is 3.910s. \n",
      "At iter 606/1000, the losses are 1.17215 (train). The time used is 3.853s. \n",
      "At iter 607/1000, the losses are 1.17217 (train). The time used is 3.716s. \n",
      "At iter 608/1000, the losses are 1.17212 (train). The time used is 3.993s. \n",
      "At iter 609/1000, the losses are 1.17214 (train). The time used is 3.877s. \n",
      "At iter 610/1000, the losses are 1.17210 (train). The time used is 3.943s. \n",
      "At iter 611/1000, the losses are 1.17211 (train). The time used is 3.825s. \n",
      "At iter 612/1000, the losses are 1.17207 (train). The time used is 3.833s. \n",
      "At iter 613/1000, the losses are 1.17207 (train). The time used is 3.809s. \n",
      "At iter 614/1000, the losses are 1.17205 (train). The time used is 3.855s. \n",
      "At iter 615/1000, the losses are 1.17204 (train). The time used is 3.802s. \n",
      "At iter 616/1000, the losses are 1.17202 (train). The time used is 3.774s. \n",
      "At iter 617/1000, the losses are 1.17201 (train). The time used is 3.808s. \n",
      "At iter 618/1000, the losses are 1.17199 (train). The time used is 3.793s. \n",
      "At iter 619/1000, the losses are 1.17198 (train). The time used is 3.832s. \n",
      "At iter 620/1000, the losses are 1.17197 (train). The time used is 3.857s. \n",
      "====================================================================================================\n",
      "At iter 620/1000, the losses on all data are 0.83944. The time used is 1.424s. \n",
      "====================================================================================================\n",
      "At iter 621/1000, the losses are 1.17196 (train). The time used is 3.886s. \n",
      "At iter 622/1000, the losses are 1.17194 (train). The time used is 3.810s. \n",
      "At iter 623/1000, the losses are 1.17193 (train). The time used is 3.784s. \n",
      "At iter 624/1000, the losses are 1.17191 (train). The time used is 4.174s. \n",
      "At iter 625/1000, the losses are 1.17190 (train). The time used is 4.433s. \n",
      "At iter 626/1000, the losses are 1.17189 (train). The time used is 4.461s. \n",
      "At iter 627/1000, the losses are 1.17187 (train). The time used is 4.616s. \n",
      "At iter 628/1000, the losses are 1.17186 (train). The time used is 4.539s. \n",
      "At iter 629/1000, the losses are 1.17184 (train). The time used is 4.442s. \n",
      "At iter 630/1000, the losses are 1.17183 (train). The time used is 4.112s. \n",
      "At iter 631/1000, the losses are 1.17182 (train). The time used is 4.671s. \n",
      "At iter 632/1000, the losses are 1.17180 (train). The time used is 4.521s. \n",
      "At iter 633/1000, the losses are 1.17179 (train). The time used is 4.496s. \n",
      "At iter 634/1000, the losses are 1.17178 (train). The time used is 4.620s. \n",
      "At iter 635/1000, the losses are 1.17176 (train). The time used is 4.621s. \n",
      "At iter 636/1000, the losses are 1.17175 (train). The time used is 4.520s. \n",
      "At iter 637/1000, the losses are 1.17173 (train). The time used is 3.763s. \n",
      "At iter 638/1000, the losses are 1.17172 (train). The time used is 4.077s. \n",
      "At iter 639/1000, the losses are 1.17171 (train). The time used is 4.344s. \n",
      "At iter 640/1000, the losses are 1.17169 (train). The time used is 3.769s. \n",
      "====================================================================================================\n",
      "At iter 640/1000, the losses on all data are 0.83945. The time used is 1.418s. \n",
      "====================================================================================================\n",
      "At iter 641/1000, the losses are 1.17168 (train). The time used is 3.741s. \n",
      "At iter 642/1000, the losses are 1.17167 (train). The time used is 3.961s. \n",
      "At iter 643/1000, the losses are 1.17165 (train). The time used is 4.293s. \n",
      "At iter 644/1000, the losses are 1.17164 (train). The time used is 4.534s. \n",
      "At iter 645/1000, the losses are 1.17162 (train). The time used is 4.649s. \n",
      "At iter 646/1000, the losses are 1.17161 (train). The time used is 4.667s. \n",
      "At iter 647/1000, the losses are 1.17160 (train). The time used is 4.122s. \n",
      "At iter 648/1000, the losses are 1.17158 (train). The time used is 4.481s. \n",
      "At iter 649/1000, the losses are 1.17157 (train). The time used is 3.855s. \n",
      "At iter 650/1000, the losses are 1.17156 (train). The time used is 3.894s. \n",
      "At iter 651/1000, the losses are 1.17154 (train). The time used is 3.798s. \n",
      "At iter 652/1000, the losses are 1.17153 (train). The time used is 3.849s. \n",
      "At iter 653/1000, the losses are 1.17151 (train). The time used is 3.987s. \n",
      "At iter 654/1000, the losses are 1.17150 (train). The time used is 3.923s. \n",
      "At iter 655/1000, the losses are 1.17149 (train). The time used is 3.876s. \n",
      "At iter 656/1000, the losses are 1.17147 (train). The time used is 3.862s. \n",
      "At iter 657/1000, the losses are 1.17146 (train). The time used is 3.813s. \n",
      "At iter 658/1000, the losses are 1.17144 (train). The time used is 3.941s. \n",
      "At iter 659/1000, the losses are 1.17143 (train). The time used is 3.966s. \n",
      "At iter 660/1000, the losses are 1.17142 (train). The time used is 4.483s. \n",
      "====================================================================================================\n",
      "At iter 660/1000, the losses on all data are 0.83946. The time used is 1.421s. \n",
      "====================================================================================================\n",
      "At iter 661/1000, the losses are 1.17140 (train). The time used is 3.909s. \n",
      "At iter 662/1000, the losses are 1.17139 (train). The time used is 4.348s. \n",
      "At iter 663/1000, the losses are 1.17137 (train). The time used is 4.665s. \n",
      "At iter 664/1000, the losses are 1.17136 (train). The time used is 4.096s. \n",
      "At iter 665/1000, the losses are 1.17135 (train). The time used is 3.864s. \n",
      "At iter 666/1000, the losses are 1.17133 (train). The time used is 4.075s. \n",
      "At iter 667/1000, the losses are 1.17132 (train). The time used is 4.728s. \n",
      "At iter 668/1000, the losses are 1.17131 (train). The time used is 4.556s. \n",
      "At iter 669/1000, the losses are 1.17129 (train). The time used is 4.536s. \n",
      "At iter 670/1000, the losses are 1.17128 (train). The time used is 4.064s. \n",
      "At iter 671/1000, the losses are 1.17126 (train). The time used is 4.465s. \n",
      "At iter 672/1000, the losses are 1.17125 (train). The time used is 4.323s. \n",
      "At iter 673/1000, the losses are 1.17124 (train). The time used is 4.103s. \n",
      "At iter 674/1000, the losses are 1.17122 (train). The time used is 4.715s. \n",
      "At iter 675/1000, the losses are 1.17121 (train). The time used is 4.398s. \n",
      "At iter 676/1000, the losses are 1.17120 (train). The time used is 3.726s. \n",
      "At iter 677/1000, the losses are 1.17118 (train). The time used is 3.798s. \n",
      "At iter 678/1000, the losses are 1.17117 (train). The time used is 3.810s. \n",
      "At iter 679/1000, the losses are 1.17116 (train). The time used is 3.827s. \n",
      "At iter 680/1000, the losses are 1.17114 (train). The time used is 3.884s. \n",
      "====================================================================================================\n",
      "At iter 680/1000, the losses on all data are 0.83948. The time used is 1.423s. \n",
      "====================================================================================================\n",
      "At iter 681/1000, the losses are 1.17113 (train). The time used is 4.449s. \n",
      "At iter 682/1000, the losses are 1.17111 (train). The time used is 4.485s. \n",
      "At iter 683/1000, the losses are 1.17110 (train). The time used is 4.561s. \n",
      "At iter 684/1000, the losses are 1.17109 (train). The time used is 4.543s. \n",
      "At iter 685/1000, the losses are 1.17107 (train). The time used is 4.289s. \n",
      "At iter 686/1000, the losses are 1.17106 (train). The time used is 3.881s. \n",
      "At iter 687/1000, the losses are 1.17105 (train). The time used is 3.828s. \n",
      "At iter 688/1000, the losses are 1.17103 (train). The time used is 4.271s. \n",
      "At iter 689/1000, the losses are 1.17102 (train). The time used is 4.077s. \n",
      "At iter 690/1000, the losses are 1.17101 (train). The time used is 4.435s. \n",
      "At iter 691/1000, the losses are 1.17099 (train). The time used is 4.003s. \n",
      "At iter 692/1000, the losses are 1.17098 (train). The time used is 3.759s. \n",
      "At iter 693/1000, the losses are 1.17096 (train). The time used is 3.816s. \n",
      "At iter 694/1000, the losses are 1.17095 (train). The time used is 4.015s. \n",
      "At iter 695/1000, the losses are 1.17094 (train). The time used is 4.360s. \n",
      "At iter 696/1000, the losses are 1.17092 (train). The time used is 4.010s. \n",
      "At iter 697/1000, the losses are 1.17091 (train). The time used is 3.746s. \n",
      "At iter 698/1000, the losses are 1.17090 (train). The time used is 3.736s. \n",
      "At iter 699/1000, the losses are 1.17088 (train). The time used is 3.798s. \n",
      "At iter 700/1000, the losses are 1.17087 (train). The time used is 3.836s. \n",
      "====================================================================================================\n",
      "At iter 700/1000, the losses on all data are 0.83949. The time used is 1.421s. \n",
      "====================================================================================================\n",
      "At iter 701/1000, the losses are 1.17086 (train). The time used is 3.850s. \n",
      "At iter 702/1000, the losses are 1.17084 (train). The time used is 3.851s. \n",
      "At iter 703/1000, the losses are 1.17083 (train). The time used is 3.793s. \n",
      "At iter 704/1000, the losses are 1.17082 (train). The time used is 3.786s. \n",
      "At iter 705/1000, the losses are 1.17080 (train). The time used is 3.854s. \n",
      "At iter 706/1000, the losses are 1.17079 (train). The time used is 3.729s. \n",
      "At iter 707/1000, the losses are 1.17078 (train). The time used is 3.926s. \n",
      "At iter 708/1000, the losses are 1.17077 (train). The time used is 4.165s. \n",
      "At iter 709/1000, the losses are 1.17076 (train). The time used is 3.835s. \n",
      "At iter 710/1000, the losses are 1.17074 (train). The time used is 3.834s. \n",
      "At iter 711/1000, the losses are 1.17073 (train). The time used is 3.864s. \n",
      "At iter 712/1000, the losses are 1.17071 (train). The time used is 3.819s. \n",
      "At iter 713/1000, the losses are 1.17070 (train). The time used is 3.892s. \n",
      "At iter 714/1000, the losses are 1.17069 (train). The time used is 3.882s. \n",
      "At iter 715/1000, the losses are 1.17068 (train). The time used is 3.920s. \n",
      "At iter 716/1000, the losses are 1.17067 (train). The time used is 3.859s. \n",
      "At iter 717/1000, the losses are 1.17065 (train). The time used is 3.824s. \n",
      "At iter 718/1000, the losses are 1.17064 (train). The time used is 3.853s. \n",
      "At iter 719/1000, the losses are 1.17062 (train). The time used is 3.875s. \n",
      "At iter 720/1000, the losses are 1.17061 (train). The time used is 3.728s. \n",
      "====================================================================================================\n",
      "At iter 720/1000, the losses on all data are 0.83952. The time used is 1.810s. \n",
      "====================================================================================================\n",
      "At iter 721/1000, the losses are 1.17060 (train). The time used is 4.549s. \n",
      "At iter 722/1000, the losses are 1.17059 (train). The time used is 4.490s. \n",
      "At iter 723/1000, the losses are 1.17057 (train). The time used is 4.568s. \n",
      "At iter 724/1000, the losses are 1.17056 (train). The time used is 4.554s. \n",
      "At iter 725/1000, the losses are 1.17055 (train). The time used is 4.536s. \n",
      "At iter 726/1000, the losses are 1.17054 (train). The time used is 4.490s. \n",
      "At iter 727/1000, the losses are 1.17052 (train). The time used is 4.190s. \n",
      "At iter 728/1000, the losses are 1.17051 (train). The time used is 3.741s. \n",
      "At iter 729/1000, the losses are 1.17050 (train). The time used is 3.961s. \n",
      "At iter 730/1000, the losses are 1.17048 (train). The time used is 3.908s. \n",
      "At iter 731/1000, the losses are 1.17047 (train). The time used is 3.803s. \n",
      "At iter 732/1000, the losses are 1.17046 (train). The time used is 3.864s. \n",
      "At iter 733/1000, the losses are 1.17044 (train). The time used is 4.342s. \n",
      "At iter 734/1000, the losses are 1.17043 (train). The time used is 4.525s. \n",
      "At iter 735/1000, the losses are 1.17042 (train). The time used is 3.837s. \n",
      "At iter 736/1000, the losses are 1.17041 (train). The time used is 3.797s. \n",
      "At iter 737/1000, the losses are 1.17040 (train). The time used is 3.865s. \n",
      "At iter 738/1000, the losses are 1.17039 (train). The time used is 3.886s. \n",
      "At iter 739/1000, the losses are 1.17038 (train). The time used is 3.840s. \n",
      "At iter 740/1000, the losses are 1.17036 (train). The time used is 3.780s. \n",
      "====================================================================================================\n",
      "At iter 740/1000, the losses on all data are 0.83954. The time used is 1.425s. \n",
      "====================================================================================================\n",
      "At iter 741/1000, the losses are 1.17035 (train). The time used is 3.846s. \n",
      "At iter 742/1000, the losses are 1.17033 (train). The time used is 3.856s. \n",
      "At iter 743/1000, the losses are 1.17032 (train). The time used is 3.882s. \n",
      "At iter 744/1000, the losses are 1.17030 (train). The time used is 4.307s. \n",
      "At iter 745/1000, the losses are 1.17029 (train). The time used is 4.500s. \n",
      "At iter 746/1000, the losses are 1.17028 (train). The time used is 4.075s. \n",
      "At iter 747/1000, the losses are 1.17027 (train). The time used is 4.297s. \n",
      "At iter 748/1000, the losses are 1.17026 (train). The time used is 4.425s. \n",
      "At iter 749/1000, the losses are 1.17025 (train). The time used is 3.815s. \n",
      "At iter 750/1000, the losses are 1.17024 (train). The time used is 3.860s. \n",
      "At iter 751/1000, the losses are 1.17022 (train). The time used is 3.837s. \n",
      "At iter 752/1000, the losses are 1.17020 (train). The time used is 3.782s. \n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "loss_cur = 0\n",
    "losses = []\n",
    "losses_test = []\n",
    "\n",
    "t0 = time.time()\n",
    "sgm_net.eval()\n",
    "loss_add = 0\n",
    "for ix in range(paras_rnn.niter):\n",
    "    rnn.train()\n",
    "    # Here because the whole dataset is not large, \n",
    "    # I use them as one batch\n",
    "    # Of course, you can use random_samples_rnn to draw \n",
    "    # X_seq = random_samples_rnn(all_data, \n",
    "    #                           batchsize=paras_rnn.batchsize)\n",
    "    X_seq = all_data_input\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    theta_pred = rnn(X_seq)\n",
    "    X_pred = sgm_net(theta_pred.flatten(0, 1))\n",
    "    loss_main = loss_fn(X_seq.flatten(0, 1).reshape(-1, 68, len(paras.freqs)),\n",
    "                   X_pred)\n",
    "    if paras_rnn.unstable_pen > 0:\n",
    "        unstable_inds = paras_stable_check(theta_pred.flatten(0, 1).detach().numpy());\n",
    "        unstable_inds = torch.tensor(unstable_inds).reshape(*theta_pred.shape[:2])\n",
    "        loss_add = (paras_rnn.unstable_pen * unstable_inds.unsqueeze(-1) * theta_pred).mean();\n",
    "    loss = loss_main + loss_add\n",
    "    \n",
    "    # Perform backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(rnn.parameters(), paras_rnn.clip)\n",
    "    # Perform optimization\n",
    "    optimizer.step()\n",
    "    \n",
    "    if ix % paras_rnn.lr_step == (paras_rnn.lr_step-1):\n",
    "        scheduler.step()\n",
    "    \n",
    "    loss_cur = loss_cur + loss_main.item()\n",
    "    if ix % paras_rnn.loss_out == (paras_rnn.loss_out-1):\n",
    "        losses.append(loss_cur/paras_rnn.loss_out)\n",
    "        print(f\"At iter {ix+1}/{paras_rnn.niter}, \"\n",
    "              f\"the losses are {loss_cur/paras_rnn.loss_out:.5f} (train). \"\n",
    "              f\"The time used is {delta_time(t0):.3f}s. \"\n",
    "             )\n",
    "        loss_cur = 0\n",
    "        t0 = time.time()\n",
    "        \n",
    "    if ix % paras_rnn.eval_out == (paras_rnn.eval_out-1):\n",
    "        rnn.eval()\n",
    "        loss_test = _evaluate(all_data).mean()\n",
    "        losses_test.append(loss_test)\n",
    "        print(f\"=\"*100)\n",
    "        print(f\"At iter {ix+1}/{paras_rnn.niter}, \"\n",
    "              f\"the losses on all data are {loss_test:.5f}. \"\n",
    "              f\"The time used is {delta_time(t0):.3f}s. \"\n",
    "             )\n",
    "        print(f\"=\"*100)\n",
    "        t0 = time.time()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041b9d90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T20:55:50.277877Z",
     "start_time": "2023-04-04T20:55:50.180639Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not RUN_PYTHON_SCRIPT:\n",
    "    plt.plot(losses[:])\n",
    "    #plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c1147e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ef8fc65",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8865f4f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T20:56:01.835052Z",
     "start_time": "2023-04-04T20:55:55.813367Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (paras_rnn.save_dir).exists():\n",
    "    trained_model = load_pkl_folder2dict(paras_rnn.save_dir)\n",
    "else:\n",
    "    trained_model = edict()\n",
    "    trained_model.model = rnn\n",
    "    trained_model.loss_fn = loss_fn\n",
    "    trained_model.optimizer = optimizer\n",
    "    trained_model.paras = paras_rnn\n",
    "    trained_model.loss = losses\n",
    "    save_pkl_dict2folder(paras_rnn.save_dir, trained_model, is_force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef95b8d8-ed67-42b4-baac-8448f6cb203f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73a978aa-d304-4743-871a-7c56d5e0a3e6",
   "metadata": {},
   "source": [
    "# PSD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8192a9-cefb-46ef-8737-199576882160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trained_model = load_pkl_folder2dict(paras_rnn.save_dir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0331a5e-1f24-4c1d-93ce-520f946eb338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b553f8-c6bb-41c8-8bdf-8348558625f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trained_model.model.eval()\n",
    "with torch.no_grad():\n",
    "    Y_pred = trained_model.model(all_data_input)\n",
    "sgm_paramss_est = Y_pred.cpu().numpy().transpose(1, 0, 2)\n",
    "trained_model.sgm_paramss_est = sgm_paramss_est\n",
    "save_pkl_dict2folder(paras_rnn.save_dir, trained_model, is_force=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca2260f-680f-4c2e-a3f7-2e65e0bfa44b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate rec PSD and save, only need once\n",
    "sgmmodel = SGM(paras.C, paras.D, paras.freqs)\n",
    "def _run_fn(sgm_param):\n",
    "    cur_PSD = sgmmodel.run_local_coupling_forward(sgm_param)\n",
    "    return cur_PSD[:68]\n",
    "X_recs = []\n",
    "for sgm_params_est in tqdm(trained_model.sgm_paramss_est):\n",
    "    if np.sum(paras_rnn.dy_mask) == 0:\n",
    "        # only for all static model\n",
    "        X_rec = _run_fn(sgm_params_est[0])\n",
    "        X_rec = np.tile(X_rec, (len(sgm_params_est), 1, 1))\n",
    "    else:\n",
    "        with Parallel(n_jobs=20) as parallel:\n",
    "            X_rec = parallel(delayed(_run_fn)(param) for param in sgm_params_est)\n",
    "    X_recs.append(X_rec)\n",
    "    \n",
    "# save\n",
    "trained_model.Rec_PSD = np.array(X_recs)\n",
    "save_pkl_dict2folder(paras_rnn.save_dir, trained_model, is_force=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e548c2-d938-4969-b462-a565cb99ae6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
