{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e873b21d",
   "metadata": {},
   "source": [
    "Here, I develope the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78809f74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T08:11:54.794807Z",
     "start_time": "2023-04-02T08:11:54.789300Z"
    },
    "execution": {
     "iopub.execute_input": "2023-04-14T18:13:50.959330Z",
     "iopub.status.busy": "2023-04-14T18:13:50.958739Z",
     "iopub.status.idle": "2023-04-14T18:13:51.118793Z",
     "shell.execute_reply": "2023-04-14T18:13:51.118143Z",
     "shell.execute_reply.started": "2023-04-14T18:13:50.959283Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../mypkg\")\n",
    "from constants import RES_ROOT, FIG_ROOT, DATA_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c9e8d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T08:11:55.137287Z",
     "start_time": "2023-04-02T08:11:55.126109Z"
    },
    "execution": {
     "iopub.execute_input": "2023-04-14T18:13:51.119832Z",
     "iopub.status.busy": "2023-04-14T18:13:51.119519Z",
     "iopub.status.idle": "2023-04-14T18:13:52.032616Z",
     "shell.execute_reply": "2023-04-14T18:13:52.031713Z",
     "shell.execute_reply.started": "2023-04-14T18:13:51.119815Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from easydict import EasyDict as edict\n",
    "from tqdm import trange, tqdm\n",
    "import time\n",
    "\n",
    "plt.style.use(FIG_ROOT/\"base.mplstyle\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdaa1032",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T08:11:55.479519Z",
     "start_time": "2023-04-02T08:11:55.442204Z"
    },
    "execution": {
     "iopub.execute_input": "2023-04-14T18:13:52.034037Z",
     "iopub.status.busy": "2023-04-14T18:13:52.033699Z",
     "iopub.status.idle": "2023-04-14T18:13:53.382496Z",
     "shell.execute_reply": "2023-04-14T18:13:53.381550Z",
     "shell.execute_reply.started": "2023-04-14T18:13:52.034020Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.lstm' from '/data/rajlab1/user_data/jin/MyResearch/TV-SGM/notebooks/../mypkg/models/lstm.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import models.lstm\n",
    "importlib.reload(models.lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb57c825",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T21:27:01.618951Z",
     "start_time": "2023-04-02T21:27:01.612910Z"
    },
    "execution": {
     "iopub.execute_input": "2023-04-14T18:13:53.384734Z",
     "iopub.status.busy": "2023-04-14T18:13:53.384012Z",
     "iopub.status.idle": "2023-04-14T18:13:53.416447Z",
     "shell.execute_reply": "2023-04-14T18:13:53.415367Z",
     "shell.execute_reply.started": "2023-04-14T18:13:53.384692Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.reparam import theta2raw_torch, raw2theta_torch, raw2theta_np\n",
    "from spectrome import Brain\n",
    "from sgm.sgm import SGM\n",
    "from utils.misc import save_pkl, save_pkl_dict2folder, load_pkl, load_pkl_folder2dict, delta_time\n",
    "from models.lstm import LSTM_SGM\n",
    "from utils.standardize import std_mat, std_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b4f6ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T08:11:57.914125Z",
     "start_time": "2023-04-02T08:11:57.906818Z"
    },
    "execution": {
     "iopub.execute_input": "2023-04-14T18:13:53.418382Z",
     "iopub.status.busy": "2023-04-14T18:13:53.417756Z",
     "iopub.status.idle": "2023-04-14T18:13:53.425266Z",
     "shell.execute_reply": "2023-04-14T18:13:53.424409Z",
     "shell.execute_reply.started": "2023-04-14T18:13:53.418342Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pkgs for pytorch ( Mar 27, 2023) \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.functional import F\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(2)\n",
    "    torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caada28d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a717cc10",
   "metadata": {},
   "source": [
    "## Data, fn and paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "858575a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T07:48:10.442850Z",
     "start_time": "2023-04-02T07:48:10.411662Z"
    },
    "execution": {
     "iopub.execute_input": "2023-04-14T18:13:56.114530Z",
     "iopub.status.busy": "2023-04-14T18:13:56.113963Z",
     "iopub.status.idle": "2023-04-14T18:13:56.145213Z",
     "shell.execute_reply": "2023-04-14T18:13:56.143889Z",
     "shell.execute_reply.started": "2023-04-14T18:13:56.114485Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the Connectome\n",
    "brain = Brain.Brain()\n",
    "brain.add_connectome(DATA_ROOT)\n",
    "brain.reorder_connectome(brain.connectome, brain.distance_matrix)\n",
    "brain.bi_symmetric_c()\n",
    "brain.reduce_extreme_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c620927",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T07:48:11.563306Z",
     "start_time": "2023-04-02T07:48:11.554387Z"
    },
    "execution": {
     "iopub.execute_input": "2023-04-14T18:13:56.762740Z",
     "iopub.status.busy": "2023-04-14T18:13:56.762206Z",
     "iopub.status.idle": "2023-04-14T18:13:56.773021Z",
     "shell.execute_reply": "2023-04-14T18:13:56.771777Z",
     "shell.execute_reply.started": "2023-04-14T18:13:56.762697Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# some constant parameters for this file\n",
    "paras = edict()\n",
    "\n",
    "## I reorder them in an alphabetical order and I change tauC to tauG (Mar 27, 2023)\n",
    "## the orginal order is taue, taui, tauC, speed, alpha, gii, gei\n",
    "## paras.par_low = np.asarray([0.005,0.005,0.005,5, 0.1,0.001,0.001])\n",
    "## paras.par_high = np.asarray([0.03, 0.20, 0.03,20,  1,    2,  0.7])\n",
    "##\n",
    "\n",
    "# alpha, gei, gii, taue, tauG, taui, speed \n",
    "paras.par_low = np.array([0.1, 0.001,0.001, 0.005, 0.005, 0.005, 5])\n",
    "paras.par_high = np.asarray([1, 0.7, 2, 0.03, 0.03, 0.20, 20])\n",
    "paras.prior_bds = np.array([paras.par_low, paras.par_high]).T\n",
    "paras.names = [\"alpha\", \"gei\", \"gii\", \"Taue\", \"TauG\", \"Taui\", \"Speed\"]\n",
    "\n",
    "paras.C = brain.reducedConnectome\n",
    "paras.D = brain.distance_matrix\n",
    "paras.freqs = np.linspace(2, 45, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54810d5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T07:48:20.016172Z",
     "start_time": "2023-04-02T07:48:19.874594Z"
    },
    "code_folding": [
     12
    ],
    "execution": {
     "iopub.execute_input": "2023-04-14T18:14:04.093992Z",
     "iopub.status.busy": "2023-04-14T18:14:04.093361Z",
     "iopub.status.idle": "2023-04-14T18:14:04.246631Z",
     "shell.execute_reply": "2023-04-14T18:14:04.245974Z",
     "shell.execute_reply.started": "2023-04-14T18:14:04.093945Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# running parameters\n",
    "# May change\n",
    "\n",
    "paras_run = edict()\n",
    "\n",
    "paras_run.n = 1000\n",
    "# note k can differ from SGM_net\n",
    "paras_run.k = 1.0 # the parameter for reparameterization in logistic\n",
    "paras_run.sd = 10 # The std to generate SGM parameters in raw scale (R)\n",
    "\n",
    "# to generate ARMA TS\n",
    "from statsmodels.tsa.arima_process import arma_generate_sample\n",
    "\n",
    "def gen_arma_ts(n, ndim=7):\n",
    "    \"\"\"Generate ARMA ndim-vec ts.\n",
    "    \"\"\"\n",
    "    # ARMA(2, 2)\n",
    "    ys = []\n",
    "    for ix in range(ndim):\n",
    "        arparams = np.array([.75, -.25])\n",
    "        maparams = np.array([.65, .35])\n",
    "        ar = np.r_[1, -arparams] # add zero-lag and negate\n",
    "        ma = np.r_[1, maparams] # add zero-lag\n",
    "        y = arma_generate_sample(ar, ma, n)\n",
    "        ys.append(y)\n",
    "    return np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f4bdb-7df0-4c90-b4e4-0c21107fee71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b782d402",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f04e7f39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T07:48:26.539383Z",
     "start_time": "2023-04-02T07:48:26.534089Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "_theta2raw = lambda Y: theta2raw_torch(Y, paras.prior_bds, paras_run.k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "734a8901",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T07:48:27.544067Z",
     "start_time": "2023-04-02T07:48:27.489177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load file /data/rajlab1/user_data/jin/MyResearch/TV-SGM/notebooks/../mypkg/../results/simu_sgm_data_conspeed_1000/PSDs.pkl\n",
      "Load file /data/rajlab1/user_data/jin/MyResearch/TV-SGM/notebooks/../mypkg/../results/simu_sgm_data_conspeed_1000/PSDs_test.pkl\n",
      "Load file /data/rajlab1/user_data/jin/MyResearch/TV-SGM/notebooks/../mypkg/../results/simu_sgm_data_conspeed_1000/sgm_paramss.pkl\n",
      "Load file /data/rajlab1/user_data/jin/MyResearch/TV-SGM/notebooks/../mypkg/../results/simu_sgm_data_conspeed_1000/sgm_paramss_test.pkl\n"
     ]
    }
   ],
   "source": [
    "simu_sgm_data = load_pkl_folder2dict(RES_ROOT/\"simu_sgm_data_conspeed_1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6aa8964c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T07:48:28.456782Z",
     "start_time": "2023-04-02T07:48:28.293550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load file /data/rajlab1/user_data/jin/MyResearch/TV-SGM/notebooks/../mypkg/../results/SGM_net/loss.pkl\n",
      "Load file /data/rajlab1/user_data/jin/MyResearch/TV-SGM/notebooks/../mypkg/../results/SGM_net/loss_test.pkl\n",
      "Load file /data/rajlab1/user_data/jin/MyResearch/TV-SGM/notebooks/../mypkg/../results/SGM_net/model.pkl\n",
      "Load file /data/rajlab1/user_data/jin/MyResearch/TV-SGM/notebooks/../mypkg/../results/SGM_net/optimizer.pkl\n",
      "Load file /data/rajlab1/user_data/jin/MyResearch/TV-SGM/notebooks/../mypkg/../results/SGM_net/paras.pkl\n"
     ]
    }
   ],
   "source": [
    "trained_model = load_pkl_folder2dict(RES_ROOT/\"SGM_net\")\n",
    "sgm_net = trained_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e45d46a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T08:12:05.376970Z",
     "start_time": "2023-04-02T08:12:05.360834Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# functions to generate training sample (Apr 1, 2023)\n",
    "def random_choice(n, batchsize=1, len_seg=None):\n",
    "    \"\"\"Randomly select the lower and upper bound of the segment\n",
    "        args:\n",
    "            n: len of the total time series\n",
    "    \"\"\"\n",
    "    if len_seg is None:\n",
    "        len_seg = torch.randint(low=10, high=200, size=(1, ))\n",
    "    up_bd = torch.randint(low=len_seg.item(), high=n, size=(batchsize, ))\n",
    "    low_bd = up_bd - len_seg\n",
    "    return low_bd, up_bd\n",
    "\n",
    "\n",
    "def random_samples_rnn(X, Y, batchsize=1, \n",
    "                       bds=None, \n",
    "                       is_std=True, \n",
    "                       theta2raw_fn=None):\n",
    "    \"\"\"Randomly select a sample from the whole segment\n",
    "        args:\n",
    "            X: PSD, num x 68 x nfreq\n",
    "            Y: params, num x 7, in orignal sgm scale\n",
    "        return:\n",
    "            X_seqs: len_seq x batchsize x num_fs\n",
    "            Y_seqs: len_seq x batchsize x 7\n",
    "            \n",
    "    \"\"\"\n",
    "    if not isinstance(X, torch.Tensor):\n",
    "        X = torch.tensor(X)\n",
    "    if is_std:\n",
    "        X = X/X.std(axis=(1, 2), keepdims=True)\n",
    "    if not isinstance(Y, torch.Tensor):\n",
    "        Y = torch.tensor(Y)\n",
    "    if theta2raw_fn: \n",
    "        Y = theta2raw_fn(Y)\n",
    "    if bds is None:\n",
    "        low_bds, up_bds = random_choice(len(X), batchsize)\n",
    "    else:\n",
    "        low_bds, up_bds = bds\n",
    "\n",
    "    X = X.flatten(1)\n",
    "    X_seqs = []\n",
    "    Y_seqs = []\n",
    "    for low_bd, up_bd in zip(low_bds, up_bds):\n",
    "        X_seq = X[low_bd:up_bd, :].unsqueeze(1)\n",
    "        Y_seq = Y[low_bd:up_bd].unsqueeze(1)\n",
    "        X_seqs.append(X_seq)\n",
    "        Y_seqs.append(Y_seq)\n",
    "    return torch.cat(X_seqs, dim=1), torch.cat(Y_seqs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "833ff922",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T08:12:06.219504Z",
     "start_time": "2023-04-02T08:12:06.212892Z"
    }
   },
   "outputs": [],
   "source": [
    "def weighted_mse_loss(pred, target, ws=None):\n",
    "    \"\"\"\n",
    "    Calculates the weighted mean squared error loss between predicted and target values.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): predicted values\n",
    "        target (torch.Tensor): target values\n",
    "        ws (torch.Tensor, optional): weights for each value. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: weighted mean squared error loss\n",
    "    \"\"\"\n",
    "    if ws is None:\n",
    "        ws = torch.ones_like(pred[0])\n",
    "        ws[:, :20] = ws[:, :20]*10\n",
    "    return torch.mean((pred-target)**2 * ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "575a41bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T08:12:07.713374Z",
     "start_time": "2023-04-02T08:12:07.706457Z"
    },
    "execution": {
     "iopub.execute_input": "2023-04-14T18:14:14.818190Z",
     "iopub.status.busy": "2023-04-14T18:14:14.817543Z",
     "iopub.status.idle": "2023-04-14T18:14:14.826314Z",
     "shell.execute_reply": "2023-04-14T18:14:14.825031Z",
     "shell.execute_reply.started": "2023-04-14T18:14:14.818142Z"
    }
   },
   "outputs": [],
   "source": [
    "paras_rnn = edict()\n",
    "paras_rnn.batchsize = 128\n",
    "paras_rnn.niter = 5000\n",
    "paras_rnn.loss_out = 10\n",
    "paras_rnn.clip = 1 # from \n",
    "paras_rnn.lr_step = 500\n",
    "\n",
    "paras_rnn.k = 1\n",
    "paras_rnn.hidden_dim = 512\n",
    "paras_rnn.output_dim = 7\n",
    "paras_rnn.input_dim = 68*40\n",
    "paras_rnn.is_bidirectional = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3314fe42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T08:12:21.752739Z",
     "start_time": "2023-04-02T08:12:21.629324Z"
    },
    "execution": {
     "iopub.execute_input": "2023-04-14T18:16:57.674168Z",
     "iopub.status.busy": "2023-04-14T18:16:57.673528Z",
     "iopub.status.idle": "2023-04-14T18:16:57.795279Z",
     "shell.execute_reply": "2023-04-14T18:16:57.794786Z",
     "shell.execute_reply.started": "2023-04-14T18:16:57.674121Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_SGM(\n",
       "  (lstm): LSTM(2720, 512, bidirectional=True)\n",
       "  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (laynorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc2): Linear(in_features=256, out_features=7, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = LSTM_SGM(input_dim=paras_rnn.input_dim, \n",
    "               hidden_dim=paras_rnn.hidden_dim, \n",
    "               output_dim=paras_rnn.output_dim, \n",
    "               is_bidirectional=paras_rnn.is_bidirectional, \n",
    "               prior_bds=torch.tensor(paras.prior_bds), \n",
    "               k = paras_rnn.k, \n",
    "               dy_mask = [1, 1, 0, 0, 0, 0, 0]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef7e0595",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T12:48:50.834434Z",
     "start_time": "2023-04-02T08:12:24.322762Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iter 10/5000, the losses are 6.10254 (train). The time used is 28.244s. \n",
      "At iter 20/5000, the losses are 2.72114 (train). The time used is 23.521s. \n",
      "At iter 30/5000, the losses are 1.74274 (train). The time used is 35.317s. \n",
      "At iter 40/5000, the losses are 1.48783 (train). The time used is 39.245s. \n",
      "At iter 50/5000, the losses are 1.39764 (train). The time used is 34.908s. \n",
      "At iter 60/5000, the losses are 1.33494 (train). The time used is 47.323s. \n",
      "At iter 70/5000, the losses are 1.26502 (train). The time used is 26.371s. \n",
      "At iter 80/5000, the losses are 1.28912 (train). The time used is 38.269s. \n",
      "At iter 90/5000, the losses are 1.29102 (train). The time used is 28.607s. \n",
      "At iter 100/5000, the losses are 1.25414 (train). The time used is 42.942s. \n",
      "At iter 110/5000, the losses are 1.26921 (train). The time used is 46.536s. \n",
      "At iter 120/5000, the losses are 1.30297 (train). The time used is 43.059s. \n",
      "At iter 130/5000, the losses are 1.23260 (train). The time used is 25.559s. \n",
      "At iter 140/5000, the losses are 1.19264 (train). The time used is 28.790s. \n",
      "At iter 150/5000, the losses are 1.20739 (train). The time used is 25.226s. \n",
      "At iter 160/5000, the losses are 1.23860 (train). The time used is 28.708s. \n",
      "At iter 170/5000, the losses are 1.22718 (train). The time used is 33.176s. \n",
      "At iter 180/5000, the losses are 1.22563 (train). The time used is 30.591s. \n",
      "At iter 190/5000, the losses are 1.20947 (train). The time used is 36.288s. \n",
      "At iter 200/5000, the losses are 1.19320 (train). The time used is 44.675s. \n",
      "At iter 210/5000, the losses are 1.16156 (train). The time used is 37.421s. \n",
      "At iter 220/5000, the losses are 1.15861 (train). The time used is 31.354s. \n",
      "At iter 230/5000, the losses are 1.15864 (train). The time used is 36.369s. \n",
      "At iter 240/5000, the losses are 1.12077 (train). The time used is 21.612s. \n",
      "At iter 250/5000, the losses are 1.15107 (train). The time used is 29.206s. \n",
      "At iter 260/5000, the losses are 1.14046 (train). The time used is 24.446s. \n",
      "At iter 270/5000, the losses are 1.17152 (train). The time used is 30.010s. \n",
      "At iter 280/5000, the losses are 1.19085 (train). The time used is 23.465s. \n",
      "At iter 290/5000, the losses are 1.16978 (train). The time used is 37.981s. \n",
      "At iter 300/5000, the losses are 1.17022 (train). The time used is 23.035s. \n",
      "At iter 310/5000, the losses are 1.16010 (train). The time used is 34.586s. \n",
      "At iter 320/5000, the losses are 1.13127 (train). The time used is 25.443s. \n",
      "At iter 330/5000, the losses are 1.17612 (train). The time used is 29.299s. \n",
      "At iter 340/5000, the losses are 1.17535 (train). The time used is 42.304s. \n",
      "At iter 350/5000, the losses are 1.15446 (train). The time used is 43.846s. \n",
      "At iter 360/5000, the losses are 1.16670 (train). The time used is 44.069s. \n",
      "At iter 370/5000, the losses are 1.14328 (train). The time used is 39.221s. \n",
      "At iter 380/5000, the losses are 1.15641 (train). The time used is 47.727s. \n",
      "At iter 390/5000, the losses are 1.12849 (train). The time used is 22.898s. \n",
      "At iter 400/5000, the losses are 1.16122 (train). The time used is 43.521s. \n",
      "At iter 410/5000, the losses are 1.12222 (train). The time used is 18.861s. \n",
      "At iter 420/5000, the losses are 1.12943 (train). The time used is 22.475s. \n",
      "At iter 430/5000, the losses are 1.11710 (train). The time used is 26.339s. \n",
      "At iter 440/5000, the losses are 1.12696 (train). The time used is 20.397s. \n",
      "At iter 450/5000, the losses are 1.11976 (train). The time used is 19.051s. \n",
      "At iter 460/5000, the losses are 1.15354 (train). The time used is 42.711s. \n",
      "At iter 470/5000, the losses are 1.12991 (train). The time used is 25.879s. \n",
      "At iter 480/5000, the losses are 1.13366 (train). The time used is 23.292s. \n",
      "At iter 490/5000, the losses are 1.11279 (train). The time used is 25.221s. \n",
      "At iter 500/5000, the losses are 1.13745 (train). The time used is 27.487s. \n",
      "Adjusting learning rate of group 0 to 1.5000e-04.\n",
      "At iter 510/5000, the losses are 1.10905 (train). The time used is 35.450s. \n",
      "At iter 520/5000, the losses are 1.10478 (train). The time used is 31.008s. \n",
      "At iter 530/5000, the losses are 1.11188 (train). The time used is 40.775s. \n",
      "At iter 540/5000, the losses are 1.09464 (train). The time used is 35.424s. \n",
      "At iter 550/5000, the losses are 1.08539 (train). The time used is 33.872s. \n",
      "At iter 560/5000, the losses are 1.10963 (train). The time used is 31.575s. \n",
      "At iter 570/5000, the losses are 1.09217 (train). The time used is 26.444s. \n",
      "At iter 580/5000, the losses are 1.07683 (train). The time used is 30.142s. \n",
      "At iter 590/5000, the losses are 1.09562 (train). The time used is 43.501s. \n",
      "At iter 600/5000, the losses are 1.09301 (train). The time used is 34.911s. \n",
      "At iter 610/5000, the losses are 1.10750 (train). The time used is 48.792s. \n",
      "At iter 620/5000, the losses are 1.08347 (train). The time used is 36.161s. \n",
      "At iter 630/5000, the losses are 1.07502 (train). The time used is 34.790s. \n",
      "At iter 640/5000, the losses are 1.09061 (train). The time used is 34.971s. \n",
      "At iter 650/5000, the losses are 1.08603 (train). The time used is 40.192s. \n",
      "At iter 660/5000, the losses are 1.09509 (train). The time used is 44.799s. \n",
      "At iter 670/5000, the losses are 1.07471 (train). The time used is 32.059s. \n",
      "At iter 680/5000, the losses are 1.09447 (train). The time used is 36.830s. \n",
      "At iter 690/5000, the losses are 1.08891 (train). The time used is 29.257s. \n",
      "At iter 700/5000, the losses are 1.08556 (train). The time used is 27.042s. \n",
      "At iter 710/5000, the losses are 1.07060 (train). The time used is 25.682s. \n",
      "At iter 720/5000, the losses are 1.09118 (train). The time used is 38.473s. \n",
      "At iter 730/5000, the losses are 1.09596 (train). The time used is 45.964s. \n",
      "At iter 740/5000, the losses are 1.06689 (train). The time used is 25.894s. \n",
      "At iter 750/5000, the losses are 1.08053 (train). The time used is 32.833s. \n",
      "At iter 760/5000, the losses are 1.09378 (train). The time used is 23.574s. \n",
      "At iter 770/5000, the losses are 1.05899 (train). The time used is 17.547s. \n",
      "At iter 780/5000, the losses are 1.08846 (train). The time used is 25.437s. \n",
      "At iter 790/5000, the losses are 1.09076 (train). The time used is 23.984s. \n",
      "At iter 800/5000, the losses are 1.07609 (train). The time used is 44.096s. \n",
      "At iter 810/5000, the losses are 1.06118 (train). The time used is 39.814s. \n",
      "At iter 820/5000, the losses are 1.08814 (train). The time used is 35.332s. \n",
      "At iter 830/5000, the losses are 1.08568 (train). The time used is 41.974s. \n",
      "At iter 840/5000, the losses are 1.06679 (train). The time used is 20.260s. \n",
      "At iter 850/5000, the losses are 1.07227 (train). The time used is 29.603s. \n",
      "At iter 860/5000, the losses are 1.08924 (train). The time used is 30.200s. \n",
      "At iter 870/5000, the losses are 1.07892 (train). The time used is 32.141s. \n",
      "At iter 880/5000, the losses are 1.08675 (train). The time used is 27.200s. \n",
      "At iter 890/5000, the losses are 1.05723 (train). The time used is 32.021s. \n",
      "At iter 900/5000, the losses are 1.09225 (train). The time used is 35.396s. \n",
      "At iter 910/5000, the losses are 1.08246 (train). The time used is 38.170s. \n",
      "At iter 920/5000, the losses are 1.06618 (train). The time used is 37.502s. \n",
      "At iter 930/5000, the losses are 1.07240 (train). The time used is 42.319s. \n",
      "At iter 940/5000, the losses are 1.06629 (train). The time used is 36.348s. \n",
      "At iter 950/5000, the losses are 1.06957 (train). The time used is 29.399s. \n",
      "At iter 960/5000, the losses are 1.07159 (train). The time used is 25.860s. \n",
      "At iter 970/5000, the losses are 1.08753 (train). The time used is 37.440s. \n",
      "At iter 980/5000, the losses are 1.07722 (train). The time used is 22.499s. \n",
      "At iter 990/5000, the losses are 1.09434 (train). The time used is 35.123s. \n",
      "At iter 1000/5000, the losses are 1.08851 (train). The time used is 30.084s. \n",
      "Adjusting learning rate of group 0 to 4.5000e-05.\n",
      "At iter 1010/5000, the losses are 1.05096 (train). The time used is 20.312s. \n",
      "At iter 1020/5000, the losses are 1.06335 (train). The time used is 22.106s. \n",
      "At iter 1030/5000, the losses are 1.07558 (train). The time used is 36.859s. \n",
      "At iter 1040/5000, the losses are 1.09311 (train). The time used is 33.776s. \n",
      "At iter 1050/5000, the losses are 1.07492 (train). The time used is 22.795s. \n",
      "At iter 1060/5000, the losses are 1.05181 (train). The time used is 29.603s. \n",
      "At iter 1070/5000, the losses are 1.07628 (train). The time used is 29.437s. \n",
      "At iter 1080/5000, the losses are 1.06143 (train). The time used is 32.141s. \n",
      "At iter 1090/5000, the losses are 1.06835 (train). The time used is 26.790s. \n",
      "At iter 1100/5000, the losses are 1.07658 (train). The time used is 36.411s. \n",
      "At iter 1110/5000, the losses are 1.06979 (train). The time used is 38.225s. \n",
      "At iter 1120/5000, the losses are 1.07145 (train). The time used is 32.568s. \n",
      "At iter 1130/5000, the losses are 1.04838 (train). The time used is 22.740s. \n",
      "At iter 1140/5000, the losses are 1.05404 (train). The time used is 12.575s. \n",
      "At iter 1150/5000, the losses are 1.07494 (train). The time used is 28.549s. \n",
      "At iter 1160/5000, the losses are 1.07668 (train). The time used is 33.189s. \n",
      "At iter 1170/5000, the losses are 1.06300 (train). The time used is 35.524s. \n",
      "At iter 1180/5000, the losses are 1.08339 (train). The time used is 32.111s. \n",
      "At iter 1190/5000, the losses are 1.07091 (train). The time used is 38.283s. \n",
      "At iter 1200/5000, the losses are 1.07952 (train). The time used is 30.167s. \n",
      "At iter 1210/5000, the losses are 1.08071 (train). The time used is 52.164s. \n",
      "At iter 1220/5000, the losses are 1.07506 (train). The time used is 31.224s. \n",
      "At iter 1230/5000, the losses are 1.07143 (train). The time used is 36.581s. \n",
      "At iter 1240/5000, the losses are 1.06457 (train). The time used is 39.607s. \n",
      "At iter 1250/5000, the losses are 1.08020 (train). The time used is 47.190s. \n",
      "At iter 1260/5000, the losses are 1.08639 (train). The time used is 41.023s. \n",
      "At iter 1270/5000, the losses are 1.05921 (train). The time used is 44.143s. \n",
      "At iter 1280/5000, the losses are 1.08186 (train). The time used is 35.605s. \n",
      "At iter 1290/5000, the losses are 1.06687 (train). The time used is 29.700s. \n",
      "At iter 1300/5000, the losses are 1.08717 (train). The time used is 52.212s. \n",
      "At iter 1310/5000, the losses are 1.07551 (train). The time used is 40.293s. \n",
      "At iter 1320/5000, the losses are 1.08688 (train). The time used is 42.183s. \n",
      "At iter 1330/5000, the losses are 1.06644 (train). The time used is 34.036s. \n",
      "At iter 1340/5000, the losses are 1.05826 (train). The time used is 23.290s. \n",
      "At iter 1350/5000, the losses are 1.05934 (train). The time used is 18.633s. \n",
      "At iter 1360/5000, the losses are 1.07769 (train). The time used is 34.307s. \n",
      "At iter 1370/5000, the losses are 1.06127 (train). The time used is 30.942s. \n",
      "At iter 1380/5000, the losses are 1.07103 (train). The time used is 32.408s. \n",
      "At iter 1390/5000, the losses are 1.06040 (train). The time used is 29.543s. \n",
      "At iter 1400/5000, the losses are 1.04581 (train). The time used is 30.625s. \n",
      "At iter 1410/5000, the losses are 1.07440 (train). The time used is 31.325s. \n",
      "At iter 1420/5000, the losses are 1.07383 (train). The time used is 29.496s. \n",
      "At iter 1430/5000, the losses are 1.06215 (train). The time used is 30.475s. \n",
      "At iter 1440/5000, the losses are 1.06026 (train). The time used is 35.515s. \n",
      "At iter 1450/5000, the losses are 1.06870 (train). The time used is 38.684s. \n",
      "At iter 1460/5000, the losses are 1.06818 (train). The time used is 36.960s. \n",
      "At iter 1470/5000, the losses are 1.06455 (train). The time used is 32.959s. \n",
      "At iter 1480/5000, the losses are 1.07593 (train). The time used is 42.700s. \n",
      "At iter 1490/5000, the losses are 1.06831 (train). The time used is 29.260s. \n",
      "At iter 1500/5000, the losses are 1.07516 (train). The time used is 31.085s. \n",
      "Adjusting learning rate of group 0 to 1.3500e-05.\n",
      "At iter 1510/5000, the losses are 1.05889 (train). The time used is 33.679s. \n",
      "At iter 1520/5000, the losses are 1.05990 (train). The time used is 31.247s. \n",
      "At iter 1530/5000, the losses are 1.05940 (train). The time used is 31.505s. \n",
      "At iter 1540/5000, the losses are 1.05924 (train). The time used is 28.142s. \n",
      "At iter 1550/5000, the losses are 1.08307 (train). The time used is 36.720s. \n",
      "At iter 1560/5000, the losses are 1.08896 (train). The time used is 34.206s. \n",
      "At iter 1570/5000, the losses are 1.06076 (train). The time used is 26.514s. \n",
      "At iter 1580/5000, the losses are 1.07290 (train). The time used is 41.736s. \n",
      "At iter 1590/5000, the losses are 1.06795 (train). The time used is 39.497s. \n",
      "At iter 1600/5000, the losses are 1.06271 (train). The time used is 22.470s. \n",
      "At iter 1610/5000, the losses are 1.06701 (train). The time used is 39.520s. \n",
      "At iter 1620/5000, the losses are 1.08544 (train). The time used is 48.295s. \n",
      "At iter 1630/5000, the losses are 1.07449 (train). The time used is 35.054s. \n",
      "At iter 1640/5000, the losses are 1.04507 (train). The time used is 27.386s. \n",
      "At iter 1650/5000, the losses are 1.06066 (train). The time used is 31.763s. \n",
      "At iter 1660/5000, the losses are 1.06951 (train). The time used is 25.508s. \n",
      "At iter 1670/5000, the losses are 1.07918 (train). The time used is 34.963s. \n",
      "At iter 1680/5000, the losses are 1.07906 (train). The time used is 41.559s. \n",
      "At iter 1690/5000, the losses are 1.04356 (train). The time used is 23.089s. \n",
      "At iter 1700/5000, the losses are 1.07356 (train). The time used is 34.044s. \n",
      "At iter 1710/5000, the losses are 1.08086 (train). The time used is 27.525s. \n",
      "At iter 1720/5000, the losses are 1.05680 (train). The time used is 22.766s. \n",
      "At iter 1730/5000, the losses are 1.05856 (train). The time used is 27.959s. \n",
      "At iter 1740/5000, the losses are 1.07045 (train). The time used is 37.376s. \n",
      "At iter 1750/5000, the losses are 1.06610 (train). The time used is 33.505s. \n",
      "At iter 1760/5000, the losses are 1.05963 (train). The time used is 29.282s. \n",
      "At iter 1770/5000, the losses are 1.07538 (train). The time used is 35.334s. \n",
      "At iter 1780/5000, the losses are 1.06466 (train). The time used is 31.042s. \n",
      "At iter 1790/5000, the losses are 1.05717 (train). The time used is 21.942s. \n",
      "At iter 1800/5000, the losses are 1.06826 (train). The time used is 31.906s. \n",
      "At iter 1810/5000, the losses are 1.06806 (train). The time used is 24.820s. \n",
      "At iter 1820/5000, the losses are 1.07363 (train). The time used is 32.230s. \n",
      "At iter 1830/5000, the losses are 1.09323 (train). The time used is 33.213s. \n",
      "At iter 1840/5000, the losses are 1.07569 (train). The time used is 28.423s. \n",
      "At iter 1850/5000, the losses are 1.06930 (train). The time used is 31.279s. \n",
      "At iter 1860/5000, the losses are 1.07195 (train). The time used is 26.574s. \n",
      "At iter 1870/5000, the losses are 1.06883 (train). The time used is 24.984s. \n",
      "At iter 1880/5000, the losses are 1.06972 (train). The time used is 30.704s. \n",
      "At iter 1890/5000, the losses are 1.08488 (train). The time used is 33.896s. \n",
      "At iter 1900/5000, the losses are 1.06492 (train). The time used is 30.714s. \n",
      "At iter 1910/5000, the losses are 1.06541 (train). The time used is 37.849s. \n",
      "At iter 1920/5000, the losses are 1.07849 (train). The time used is 48.315s. \n",
      "At iter 1930/5000, the losses are 1.08073 (train). The time used is 29.803s. \n",
      "At iter 1940/5000, the losses are 1.07213 (train). The time used is 36.691s. \n",
      "At iter 1950/5000, the losses are 1.07712 (train). The time used is 32.532s. \n",
      "At iter 1960/5000, the losses are 1.05547 (train). The time used is 35.632s. \n",
      "At iter 1970/5000, the losses are 1.08050 (train). The time used is 35.024s. \n",
      "At iter 1980/5000, the losses are 1.07868 (train). The time used is 42.032s. \n",
      "At iter 1990/5000, the losses are 1.05591 (train). The time used is 32.780s. \n",
      "At iter 2000/5000, the losses are 1.07320 (train). The time used is 22.571s. \n",
      "Adjusting learning rate of group 0 to 4.0500e-06.\n",
      "At iter 2010/5000, the losses are 1.08271 (train). The time used is 27.431s. \n",
      "At iter 2020/5000, the losses are 1.05956 (train). The time used is 33.438s. \n",
      "At iter 2030/5000, the losses are 1.06859 (train). The time used is 38.642s. \n",
      "At iter 2040/5000, the losses are 1.05652 (train). The time used is 29.066s. \n",
      "At iter 2050/5000, the losses are 1.07332 (train). The time used is 28.112s. \n",
      "At iter 2060/5000, the losses are 1.04377 (train). The time used is 22.171s. \n",
      "At iter 2070/5000, the losses are 1.07934 (train). The time used is 34.491s. \n",
      "At iter 2080/5000, the losses are 1.07401 (train). The time used is 33.790s. \n",
      "At iter 2090/5000, the losses are 1.05806 (train). The time used is 24.151s. \n",
      "At iter 2100/5000, the losses are 1.07549 (train). The time used is 29.832s. \n",
      "At iter 2110/5000, the losses are 1.08204 (train). The time used is 28.812s. \n",
      "At iter 2120/5000, the losses are 1.04052 (train). The time used is 22.610s. \n",
      "At iter 2130/5000, the losses are 1.05641 (train). The time used is 23.931s. \n",
      "At iter 2140/5000, the losses are 1.06333 (train). The time used is 31.057s. \n",
      "At iter 2150/5000, the losses are 1.06098 (train). The time used is 38.963s. \n",
      "At iter 2160/5000, the losses are 1.06571 (train). The time used is 35.910s. \n",
      "At iter 2170/5000, the losses are 1.07552 (train). The time used is 34.016s. \n",
      "At iter 2180/5000, the losses are 1.07904 (train). The time used is 31.569s. \n",
      "At iter 2190/5000, the losses are 1.04540 (train). The time used is 23.622s. \n",
      "At iter 2200/5000, the losses are 1.07666 (train). The time used is 34.691s. \n",
      "At iter 2210/5000, the losses are 1.07918 (train). The time used is 43.153s. \n",
      "At iter 2220/5000, the losses are 1.05339 (train). The time used is 25.272s. \n",
      "At iter 2230/5000, the losses are 1.06200 (train). The time used is 28.311s. \n",
      "At iter 2240/5000, the losses are 1.06769 (train). The time used is 33.308s. \n",
      "At iter 2250/5000, the losses are 1.07608 (train). The time used is 32.210s. \n",
      "At iter 2260/5000, the losses are 1.05452 (train). The time used is 34.188s. \n",
      "At iter 2270/5000, the losses are 1.09389 (train). The time used is 45.877s. \n",
      "At iter 2280/5000, the losses are 1.06565 (train). The time used is 41.088s. \n",
      "At iter 2290/5000, the losses are 1.05255 (train). The time used is 24.480s. \n",
      "At iter 2300/5000, the losses are 1.08587 (train). The time used is 45.211s. \n",
      "At iter 2310/5000, the losses are 1.05792 (train). The time used is 29.164s. \n",
      "At iter 2320/5000, the losses are 1.07570 (train). The time used is 44.443s. \n",
      "At iter 2330/5000, the losses are 1.07129 (train). The time used is 32.558s. \n",
      "At iter 2340/5000, the losses are 1.06326 (train). The time used is 30.987s. \n",
      "At iter 2350/5000, the losses are 1.07177 (train). The time used is 33.015s. \n",
      "At iter 2360/5000, the losses are 1.04479 (train). The time used is 30.396s. \n",
      "At iter 2370/5000, the losses are 1.07036 (train). The time used is 44.288s. \n",
      "At iter 2380/5000, the losses are 1.06671 (train). The time used is 33.712s. \n",
      "At iter 2390/5000, the losses are 1.05919 (train). The time used is 34.114s. \n",
      "At iter 2400/5000, the losses are 1.06427 (train). The time used is 40.788s. \n",
      "At iter 2410/5000, the losses are 1.05505 (train). The time used is 26.494s. \n",
      "At iter 2420/5000, the losses are 1.04708 (train). The time used is 25.165s. \n",
      "At iter 2430/5000, the losses are 1.07822 (train). The time used is 39.570s. \n",
      "At iter 2440/5000, the losses are 1.08701 (train). The time used is 31.524s. \n",
      "At iter 2450/5000, the losses are 1.06008 (train). The time used is 23.764s. \n",
      "At iter 2460/5000, the losses are 1.08037 (train). The time used is 40.943s. \n",
      "At iter 2470/5000, the losses are 1.05670 (train). The time used is 28.278s. \n",
      "At iter 2480/5000, the losses are 1.05826 (train). The time used is 36.585s. \n",
      "At iter 2490/5000, the losses are 1.06539 (train). The time used is 39.988s. \n",
      "At iter 2500/5000, the losses are 1.05913 (train). The time used is 38.418s. \n",
      "Adjusting learning rate of group 0 to 1.2150e-06.\n",
      "At iter 2510/5000, the losses are 1.09010 (train). The time used is 51.384s. \n",
      "At iter 2520/5000, the losses are 1.08879 (train). The time used is 38.736s. \n",
      "At iter 2530/5000, the losses are 1.07998 (train). The time used is 47.988s. \n",
      "At iter 2540/5000, the losses are 1.05660 (train). The time used is 18.002s. \n",
      "At iter 2550/5000, the losses are 1.05183 (train). The time used is 25.539s. \n",
      "At iter 2560/5000, the losses are 1.05181 (train). The time used is 32.362s. \n",
      "At iter 2570/5000, the losses are 1.07073 (train). The time used is 25.706s. \n",
      "At iter 2580/5000, the losses are 1.06375 (train). The time used is 32.852s. \n",
      "At iter 2590/5000, the losses are 1.06059 (train). The time used is 28.075s. \n",
      "At iter 2600/5000, the losses are 1.06784 (train). The time used is 39.159s. \n",
      "At iter 2610/5000, the losses are 1.05432 (train). The time used is 33.729s. \n",
      "At iter 2620/5000, the losses are 1.04357 (train). The time used is 27.800s. \n",
      "At iter 2630/5000, the losses are 1.04644 (train). The time used is 28.302s. \n",
      "At iter 2640/5000, the losses are 1.05772 (train). The time used is 32.799s. \n",
      "At iter 2650/5000, the losses are 1.05959 (train). The time used is 24.108s. \n",
      "At iter 2660/5000, the losses are 1.06176 (train). The time used is 38.208s. \n",
      "At iter 2670/5000, the losses are 1.06248 (train). The time used is 39.457s. \n",
      "At iter 2680/5000, the losses are 1.07288 (train). The time used is 44.881s. \n",
      "At iter 2690/5000, the losses are 1.07789 (train). The time used is 31.661s. \n",
      "At iter 2700/5000, the losses are 1.06234 (train). The time used is 22.986s. \n",
      "At iter 2710/5000, the losses are 1.08105 (train). The time used is 42.129s. \n",
      "At iter 2720/5000, the losses are 1.06267 (train). The time used is 36.222s. \n",
      "At iter 2730/5000, the losses are 1.05246 (train). The time used is 30.091s. \n",
      "At iter 2740/5000, the losses are 1.06926 (train). The time used is 25.380s. \n",
      "At iter 2750/5000, the losses are 1.06220 (train). The time used is 32.979s. \n",
      "At iter 2760/5000, the losses are 1.04940 (train). The time used is 27.005s. \n",
      "At iter 2770/5000, the losses are 1.05837 (train). The time used is 32.419s. \n",
      "At iter 2780/5000, the losses are 1.07124 (train). The time used is 33.553s. \n",
      "At iter 2790/5000, the losses are 1.07323 (train). The time used is 46.730s. \n",
      "At iter 2800/5000, the losses are 1.05205 (train). The time used is 23.871s. \n",
      "At iter 2810/5000, the losses are 1.06789 (train). The time used is 29.231s. \n",
      "At iter 2820/5000, the losses are 1.08413 (train). The time used is 39.822s. \n",
      "At iter 2830/5000, the losses are 1.06397 (train). The time used is 32.338s. \n",
      "At iter 2840/5000, the losses are 1.05691 (train). The time used is 46.204s. \n",
      "At iter 2850/5000, the losses are 1.08092 (train). The time used is 50.835s. \n",
      "At iter 2860/5000, the losses are 1.07847 (train). The time used is 41.522s. \n",
      "At iter 2870/5000, the losses are 1.04642 (train). The time used is 19.833s. \n",
      "At iter 2880/5000, the losses are 1.06922 (train). The time used is 30.498s. \n",
      "At iter 2890/5000, the losses are 1.07484 (train). The time used is 30.338s. \n",
      "At iter 2900/5000, the losses are 1.05397 (train). The time used is 25.828s. \n",
      "At iter 2910/5000, the losses are 1.03790 (train). The time used is 18.373s. \n",
      "At iter 2920/5000, the losses are 1.07221 (train). The time used is 19.140s. \n",
      "At iter 2930/5000, the losses are 1.07166 (train). The time used is 30.224s. \n",
      "At iter 2940/5000, the losses are 1.05460 (train). The time used is 19.436s. \n",
      "At iter 2950/5000, the losses are 1.06600 (train). The time used is 24.866s. \n",
      "At iter 2960/5000, the losses are 1.06258 (train). The time used is 30.552s. \n",
      "At iter 2970/5000, the losses are 1.06739 (train). The time used is 26.646s. \n",
      "At iter 2980/5000, the losses are 1.05214 (train). The time used is 28.720s. \n",
      "At iter 2990/5000, the losses are 1.05662 (train). The time used is 23.144s. \n",
      "At iter 3000/5000, the losses are 1.08041 (train). The time used is 30.648s. \n",
      "Adjusting learning rate of group 0 to 3.6450e-07.\n",
      "At iter 3010/5000, the losses are 1.06623 (train). The time used is 34.307s. \n",
      "At iter 3020/5000, the losses are 1.07099 (train). The time used is 34.953s. \n",
      "At iter 3030/5000, the losses are 1.05461 (train). The time used is 26.325s. \n",
      "At iter 3040/5000, the losses are 1.06795 (train). The time used is 39.894s. \n",
      "At iter 3050/5000, the losses are 1.05756 (train). The time used is 29.240s. \n",
      "At iter 3060/5000, the losses are 1.07757 (train). The time used is 42.576s. \n",
      "At iter 3070/5000, the losses are 1.07530 (train). The time used is 47.115s. \n",
      "At iter 3080/5000, the losses are 1.06307 (train). The time used is 39.940s. \n",
      "At iter 3090/5000, the losses are 1.07757 (train). The time used is 44.736s. \n",
      "At iter 3100/5000, the losses are 1.07690 (train). The time used is 32.118s. \n",
      "At iter 3110/5000, the losses are 1.06239 (train). The time used is 42.633s. \n",
      "At iter 3120/5000, the losses are 1.05442 (train). The time used is 42.605s. \n",
      "At iter 3130/5000, the losses are 1.07286 (train). The time used is 39.705s. \n",
      "At iter 3140/5000, the losses are 1.07962 (train). The time used is 31.860s. \n",
      "At iter 3150/5000, the losses are 1.07991 (train). The time used is 38.354s. \n",
      "At iter 3160/5000, the losses are 1.06630 (train). The time used is 30.830s. \n",
      "At iter 3170/5000, the losses are 1.06717 (train). The time used is 22.428s. \n",
      "At iter 3180/5000, the losses are 1.05427 (train). The time used is 26.234s. \n",
      "At iter 3190/5000, the losses are 1.08290 (train). The time used is 25.435s. \n",
      "At iter 3200/5000, the losses are 1.04325 (train). The time used is 18.417s. \n",
      "At iter 3210/5000, the losses are 1.07581 (train). The time used is 25.020s. \n",
      "At iter 3220/5000, the losses are 1.06550 (train). The time used is 27.253s. \n",
      "At iter 3230/5000, the losses are 1.06754 (train). The time used is 27.101s. \n",
      "At iter 3240/5000, the losses are 1.07341 (train). The time used is 39.556s. \n",
      "At iter 3250/5000, the losses are 1.07258 (train). The time used is 34.379s. \n",
      "At iter 3260/5000, the losses are 1.06350 (train). The time used is 32.174s. \n",
      "At iter 3270/5000, the losses are 1.07827 (train). The time used is 43.187s. \n",
      "At iter 3280/5000, the losses are 1.07656 (train). The time used is 43.438s. \n",
      "At iter 3290/5000, the losses are 1.07055 (train). The time used is 25.632s. \n",
      "At iter 3300/5000, the losses are 1.07911 (train). The time used is 29.481s. \n",
      "At iter 3310/5000, the losses are 1.05474 (train). The time used is 31.326s. \n",
      "At iter 3320/5000, the losses are 1.06798 (train). The time used is 40.008s. \n",
      "At iter 3330/5000, the losses are 1.06894 (train). The time used is 35.512s. \n",
      "At iter 3340/5000, the losses are 1.06422 (train). The time used is 40.025s. \n",
      "At iter 3350/5000, the losses are 1.07283 (train). The time used is 39.550s. \n",
      "At iter 3360/5000, the losses are 1.08275 (train). The time used is 41.023s. \n",
      "At iter 3370/5000, the losses are 1.06401 (train). The time used is 27.536s. \n",
      "At iter 3380/5000, the losses are 1.05900 (train). The time used is 24.852s. \n",
      "At iter 3390/5000, the losses are 1.06798 (train). The time used is 34.880s. \n",
      "At iter 3400/5000, the losses are 1.08101 (train). The time used is 50.182s. \n",
      "At iter 3410/5000, the losses are 1.07816 (train). The time used is 44.260s. \n",
      "At iter 3420/5000, the losses are 1.07674 (train). The time used is 46.553s. \n",
      "At iter 3430/5000, the losses are 1.07206 (train). The time used is 37.236s. \n",
      "At iter 3440/5000, the losses are 1.07747 (train). The time used is 36.376s. \n",
      "At iter 3450/5000, the losses are 1.06632 (train). The time used is 46.193s. \n",
      "At iter 3460/5000, the losses are 1.04630 (train). The time used is 31.644s. \n",
      "At iter 3470/5000, the losses are 1.06605 (train). The time used is 38.812s. \n",
      "At iter 3480/5000, the losses are 1.08097 (train). The time used is 34.347s. \n",
      "At iter 3490/5000, the losses are 1.06097 (train). The time used is 29.283s. \n",
      "At iter 3500/5000, the losses are 1.06652 (train). The time used is 37.005s. \n",
      "Adjusting learning rate of group 0 to 1.0935e-07.\n",
      "At iter 3510/5000, the losses are 1.06833 (train). The time used is 34.725s. \n",
      "At iter 3520/5000, the losses are 1.06185 (train). The time used is 27.322s. \n",
      "At iter 3530/5000, the losses are 1.07190 (train). The time used is 30.562s. \n",
      "At iter 3540/5000, the losses are 1.07944 (train). The time used is 36.415s. \n",
      "At iter 3550/5000, the losses are 1.06997 (train). The time used is 29.492s. \n",
      "At iter 3560/5000, the losses are 1.07710 (train). The time used is 35.121s. \n",
      "At iter 3570/5000, the losses are 1.07224 (train). The time used is 31.457s. \n",
      "At iter 3580/5000, the losses are 1.06254 (train). The time used is 34.077s. \n",
      "At iter 3590/5000, the losses are 1.07395 (train). The time used is 29.249s. \n",
      "At iter 3600/5000, the losses are 1.05154 (train). The time used is 32.890s. \n",
      "At iter 3610/5000, the losses are 1.05943 (train). The time used is 23.433s. \n",
      "At iter 3620/5000, the losses are 1.07064 (train). The time used is 31.685s. \n",
      "At iter 3630/5000, the losses are 1.05305 (train). The time used is 24.172s. \n",
      "At iter 3640/5000, the losses are 1.06702 (train). The time used is 24.238s. \n",
      "At iter 3650/5000, the losses are 1.06601 (train). The time used is 34.968s. \n",
      "At iter 3660/5000, the losses are 1.06182 (train). The time used is 30.908s. \n",
      "At iter 3670/5000, the losses are 1.06172 (train). The time used is 27.872s. \n",
      "At iter 3680/5000, the losses are 1.05950 (train). The time used is 30.964s. \n",
      "At iter 3690/5000, the losses are 1.09732 (train). The time used is 40.423s. \n",
      "At iter 3700/5000, the losses are 1.05022 (train). The time used is 28.451s. \n",
      "At iter 3710/5000, the losses are 1.06542 (train). The time used is 36.496s. \n",
      "At iter 3720/5000, the losses are 1.07728 (train). The time used is 36.886s. \n",
      "At iter 3730/5000, the losses are 1.08109 (train). The time used is 40.973s. \n",
      "At iter 3740/5000, the losses are 1.06559 (train). The time used is 37.832s. \n",
      "At iter 3750/5000, the losses are 1.07931 (train). The time used is 38.503s. \n",
      "At iter 3760/5000, the losses are 1.07205 (train). The time used is 40.290s. \n",
      "At iter 3770/5000, the losses are 1.05275 (train). The time used is 23.926s. \n",
      "At iter 3780/5000, the losses are 1.05789 (train). The time used is 27.733s. \n",
      "At iter 3790/5000, the losses are 1.05716 (train). The time used is 30.696s. \n",
      "At iter 3800/5000, the losses are 1.06669 (train). The time used is 49.590s. \n",
      "At iter 3810/5000, the losses are 1.08953 (train). The time used is 42.250s. \n",
      "At iter 3820/5000, the losses are 1.06392 (train). The time used is 28.310s. \n",
      "At iter 3830/5000, the losses are 1.07580 (train). The time used is 37.015s. \n",
      "At iter 3840/5000, the losses are 1.07808 (train). The time used is 29.909s. \n",
      "At iter 3850/5000, the losses are 1.08009 (train). The time used is 28.147s. \n",
      "At iter 3860/5000, the losses are 1.07114 (train). The time used is 36.752s. \n",
      "At iter 3870/5000, the losses are 1.07781 (train). The time used is 35.107s. \n",
      "At iter 3880/5000, the losses are 1.06498 (train). The time used is 32.115s. \n",
      "At iter 3890/5000, the losses are 1.07672 (train). The time used is 34.020s. \n",
      "At iter 3900/5000, the losses are 1.04889 (train). The time used is 30.614s. \n",
      "At iter 3910/5000, the losses are 1.07687 (train). The time used is 43.573s. \n",
      "At iter 3920/5000, the losses are 1.05150 (train). The time used is 25.016s. \n",
      "At iter 3930/5000, the losses are 1.03742 (train). The time used is 23.265s. \n",
      "At iter 3940/5000, the losses are 1.04818 (train). The time used is 35.114s. \n",
      "At iter 3950/5000, the losses are 1.06195 (train). The time used is 35.363s. \n",
      "At iter 3960/5000, the losses are 1.05143 (train). The time used is 24.330s. \n",
      "At iter 3970/5000, the losses are 1.06510 (train). The time used is 25.669s. \n",
      "At iter 3980/5000, the losses are 1.06513 (train). The time used is 24.335s. \n",
      "At iter 3990/5000, the losses are 1.06661 (train). The time used is 28.830s. \n",
      "At iter 4000/5000, the losses are 1.06609 (train). The time used is 29.744s. \n",
      "Adjusting learning rate of group 0 to 3.2805e-08.\n",
      "At iter 4010/5000, the losses are 1.07438 (train). The time used is 38.750s. \n",
      "At iter 4020/5000, the losses are 1.07220 (train). The time used is 38.236s. \n",
      "At iter 4030/5000, the losses are 1.05113 (train). The time used is 26.921s. \n",
      "At iter 4040/5000, the losses are 1.06764 (train). The time used is 25.879s. \n",
      "At iter 4050/5000, the losses are 1.06886 (train). The time used is 42.765s. \n",
      "At iter 4060/5000, the losses are 1.08401 (train). The time used is 42.714s. \n",
      "At iter 4070/5000, the losses are 1.05698 (train). The time used is 27.589s. \n",
      "At iter 4080/5000, the losses are 1.07195 (train). The time used is 37.700s. \n",
      "At iter 4090/5000, the losses are 1.04793 (train). The time used is 20.595s. \n",
      "At iter 4100/5000, the losses are 1.03383 (train). The time used is 28.156s. \n",
      "At iter 4110/5000, the losses are 1.07917 (train). The time used is 25.954s. \n",
      "At iter 4120/5000, the losses are 1.03835 (train). The time used is 29.908s. \n",
      "At iter 4130/5000, the losses are 1.06695 (train). The time used is 34.759s. \n",
      "At iter 4140/5000, the losses are 1.05664 (train). The time used is 34.542s. \n",
      "At iter 4150/5000, the losses are 1.07312 (train). The time used is 38.751s. \n",
      "At iter 4160/5000, the losses are 1.06888 (train). The time used is 31.019s. \n",
      "At iter 4170/5000, the losses are 1.06941 (train). The time used is 28.711s. \n",
      "At iter 4180/5000, the losses are 1.06194 (train). The time used is 31.848s. \n",
      "At iter 4190/5000, the losses are 1.06631 (train). The time used is 33.046s. \n",
      "At iter 4200/5000, the losses are 1.06750 (train). The time used is 41.213s. \n",
      "At iter 4210/5000, the losses are 1.07103 (train). The time used is 25.990s. \n",
      "At iter 4220/5000, the losses are 1.07626 (train). The time used is 29.140s. \n",
      "At iter 4230/5000, the losses are 1.06295 (train). The time used is 35.368s. \n",
      "At iter 4240/5000, the losses are 1.06548 (train). The time used is 28.330s. \n",
      "At iter 4250/5000, the losses are 1.03690 (train). The time used is 29.113s. \n",
      "At iter 4260/5000, the losses are 1.06195 (train). The time used is 33.413s. \n",
      "At iter 4270/5000, the losses are 1.07832 (train). The time used is 28.967s. \n",
      "At iter 4280/5000, the losses are 1.07599 (train). The time used is 48.043s. \n",
      "At iter 4290/5000, the losses are 1.05712 (train). The time used is 21.573s. \n",
      "At iter 4300/5000, the losses are 1.06392 (train). The time used is 39.235s. \n",
      "At iter 4310/5000, the losses are 1.07676 (train). The time used is 34.874s. \n",
      "At iter 4320/5000, the losses are 1.07868 (train). The time used is 41.705s. \n",
      "At iter 4330/5000, the losses are 1.04425 (train). The time used is 23.589s. \n",
      "At iter 4340/5000, the losses are 1.06720 (train). The time used is 29.350s. \n",
      "At iter 4350/5000, the losses are 1.06603 (train). The time used is 32.579s. \n",
      "At iter 4360/5000, the losses are 1.07904 (train). The time used is 35.777s. \n",
      "At iter 4370/5000, the losses are 1.07620 (train). The time used is 28.431s. \n",
      "At iter 4380/5000, the losses are 1.06825 (train). The time used is 38.566s. \n",
      "At iter 4390/5000, the losses are 1.06703 (train). The time used is 34.168s. \n",
      "At iter 4400/5000, the losses are 1.08668 (train). The time used is 37.869s. \n",
      "At iter 4410/5000, the losses are 1.07201 (train). The time used is 26.794s. \n",
      "At iter 4420/5000, the losses are 1.06368 (train). The time used is 35.588s. \n",
      "At iter 4430/5000, the losses are 1.08029 (train). The time used is 40.801s. \n",
      "At iter 4440/5000, the losses are 1.05553 (train). The time used is 29.575s. \n",
      "At iter 4450/5000, the losses are 1.06492 (train). The time used is 40.532s. \n",
      "At iter 4460/5000, the losses are 1.07219 (train). The time used is 37.552s. \n",
      "At iter 4470/5000, the losses are 1.08075 (train). The time used is 32.542s. \n",
      "At iter 4480/5000, the losses are 1.06878 (train). The time used is 46.509s. \n",
      "At iter 4490/5000, the losses are 1.06177 (train). The time used is 28.349s. \n",
      "At iter 4500/5000, the losses are 1.07759 (train). The time used is 42.187s. \n",
      "Adjusting learning rate of group 0 to 9.8415e-09.\n",
      "At iter 4510/5000, the losses are 1.03896 (train). The time used is 35.214s. \n",
      "At iter 4520/5000, the losses are 1.07249 (train). The time used is 39.794s. \n",
      "At iter 4530/5000, the losses are 1.07066 (train). The time used is 35.054s. \n",
      "At iter 4540/5000, the losses are 1.07372 (train). The time used is 30.491s. \n",
      "At iter 4550/5000, the losses are 1.08229 (train). The time used is 42.742s. \n",
      "At iter 4560/5000, the losses are 1.05710 (train). The time used is 23.484s. \n",
      "At iter 4570/5000, the losses are 1.08152 (train). The time used is 47.709s. \n",
      "At iter 4580/5000, the losses are 1.06674 (train). The time used is 36.565s. \n",
      "At iter 4590/5000, the losses are 1.07055 (train). The time used is 28.440s. \n",
      "At iter 4600/5000, the losses are 1.06480 (train). The time used is 34.831s. \n",
      "At iter 4610/5000, the losses are 1.06002 (train). The time used is 21.853s. \n",
      "At iter 4620/5000, the losses are 1.06986 (train). The time used is 35.431s. \n",
      "At iter 4630/5000, the losses are 1.07706 (train). The time used is 41.314s. \n",
      "At iter 4640/5000, the losses are 1.07416 (train). The time used is 35.743s. \n",
      "At iter 4650/5000, the losses are 1.07057 (train). The time used is 28.898s. \n",
      "At iter 4660/5000, the losses are 1.07572 (train). The time used is 41.473s. \n",
      "At iter 4670/5000, the losses are 1.07429 (train). The time used is 34.408s. \n",
      "At iter 4680/5000, the losses are 1.05750 (train). The time used is 23.510s. \n",
      "At iter 4690/5000, the losses are 1.08584 (train). The time used is 40.346s. \n",
      "At iter 4700/5000, the losses are 1.07653 (train). The time used is 31.043s. \n",
      "At iter 4710/5000, the losses are 1.07555 (train). The time used is 44.913s. \n",
      "At iter 4720/5000, the losses are 1.07491 (train). The time used is 28.901s. \n",
      "At iter 4730/5000, the losses are 1.06059 (train). The time used is 32.204s. \n",
      "At iter 4740/5000, the losses are 1.06915 (train). The time used is 40.855s. \n",
      "At iter 4750/5000, the losses are 1.06911 (train). The time used is 46.694s. \n",
      "At iter 4760/5000, the losses are 1.06818 (train). The time used is 40.879s. \n",
      "At iter 4770/5000, the losses are 1.06293 (train). The time used is 39.365s. \n",
      "At iter 4780/5000, the losses are 1.08196 (train). The time used is 33.418s. \n",
      "At iter 4790/5000, the losses are 1.06644 (train). The time used is 29.615s. \n",
      "At iter 4800/5000, the losses are 1.07761 (train). The time used is 29.025s. \n",
      "At iter 4810/5000, the losses are 1.08719 (train). The time used is 34.227s. \n",
      "At iter 4820/5000, the losses are 1.06886 (train). The time used is 30.573s. \n",
      "At iter 4830/5000, the losses are 1.07028 (train). The time used is 52.772s. \n",
      "At iter 4840/5000, the losses are 1.09942 (train). The time used is 47.218s. \n",
      "At iter 4850/5000, the losses are 1.05303 (train). The time used is 33.842s. \n",
      "At iter 4860/5000, the losses are 1.08387 (train). The time used is 41.019s. \n",
      "At iter 4870/5000, the losses are 1.07671 (train). The time used is 35.069s. \n",
      "At iter 4880/5000, the losses are 1.05107 (train). The time used is 21.520s. \n",
      "At iter 4890/5000, the losses are 1.06497 (train). The time used is 32.752s. \n",
      "At iter 4900/5000, the losses are 1.07904 (train). The time used is 41.765s. \n",
      "At iter 4910/5000, the losses are 1.06482 (train). The time used is 22.221s. \n",
      "At iter 4920/5000, the losses are 1.05621 (train). The time used is 27.667s. \n",
      "At iter 4930/5000, the losses are 1.05929 (train). The time used is 41.615s. \n",
      "At iter 4940/5000, the losses are 1.06625 (train). The time used is 38.674s. \n",
      "At iter 4950/5000, the losses are 1.06238 (train). The time used is 29.656s. \n",
      "At iter 4960/5000, the losses are 1.08780 (train). The time used is 40.253s. \n",
      "At iter 4970/5000, the losses are 1.06960 (train). The time used is 34.907s. \n",
      "At iter 4980/5000, the losses are 1.04771 (train). The time used is 28.479s. \n",
      "At iter 4990/5000, the losses are 1.07345 (train). The time used is 37.307s. \n",
      "At iter 5000/5000, the losses are 1.05017 (train). The time used is 23.857s. \n",
      "Adjusting learning rate of group 0 to 2.9524e-09.\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "loss_cur = 0\n",
    "losses = []\n",
    "losses_test = []\n",
    "\n",
    "t0 = time.time()\n",
    "sgm_net.eval()\n",
    "for ix in range(paras_rnn.niter):\n",
    "    X_seq, Y_seq = random_samples_rnn(simu_sgm_data.PSDs,\n",
    "                                      simu_sgm_data.sgm_paramss, \n",
    "                                      batchsize=paras_rnn.batchsize, \n",
    "                                      theta2raw_fn=_theta2raw)\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    theta_pred = rnn(X_seq)\n",
    "    X_pred = sgm_net(theta_pred.flatten(0, 1))\n",
    "    loss = loss_fn(X_seq.flatten(0, 1).reshape(-1, 68, 40),\n",
    "                   X_pred)\n",
    "    \n",
    "    # Perform backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(rnn.parameters(), paras_rnn.clip)\n",
    "    # Perform optimization\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_cur = loss_cur + loss.item()\n",
    "    if ix % paras_rnn.loss_out == (paras_rnn.loss_out-1):\n",
    "        losses.append(loss_cur/paras_rnn.loss_out)\n",
    "        print(f\"At iter {ix+1}/{paras_rnn.niter}, \"\n",
    "              f\"the losses are {loss_cur/paras_rnn.loss_out:.5f} (train). \"\n",
    "              f\"The time used is {delta_time(t0):.3f}s. \"\n",
    "             )\n",
    "        loss_cur = 0\n",
    "        rnn.train()\n",
    "        t0 = time.time()\n",
    "    \n",
    "    if ix % paras_rnn.lr_step == (paras_rnn.lr_step-1):\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "008687a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T21:07:09.849482Z",
     "start_time": "2023-04-02T21:07:09.605859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce4774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
