Running prime number generator program on 30 CPU cores
Load file /data/rajlab1/user_data/jin/MyResearch/TV-SGM/bash_scripts/../mypkg/../results/simu_sgm_data_ind_large/PSDs.pkl
Load file /data/rajlab1/user_data/jin/MyResearch/TV-SGM/bash_scripts/../mypkg/../results/simu_sgm_data_ind_large/PSDs_test.pkl
Load file /data/rajlab1/user_data/jin/MyResearch/TV-SGM/bash_scripts/../mypkg/../results/simu_sgm_data_ind_large/freqs.pkl
Load file /data/rajlab1/user_data/jin/MyResearch/TV-SGM/bash_scripts/../mypkg/../results/simu_sgm_data_ind_large/sgm_paramss.pkl
Load file /data/rajlab1/user_data/jin/MyResearch/TV-SGM/bash_scripts/../mypkg/../results/simu_sgm_data_ind_large/sgm_paramss_test.pkl
Adjusting learning rate of group 0 to 1.0000e-03.
At epoch 20/1200,the losses are 2.60877 (train) and 0.19323 (test). The time used is 122.143s. 
At epoch 40/1200,the losses are 1.18592 (train) and 0.07017 (test). The time used is 115.687s. 
At epoch 60/1200,the losses are 0.60361 (train) and 0.09740 (test). The time used is 118.178s. 
At epoch 80/1200,the losses are 0.50732 (train) and 0.04056 (test). The time used is 116.794s. 
At epoch 100/1200,the losses are 0.44181 (train) and 0.04589 (test). The time used is 116.861s. 
At epoch 120/1200,the losses are 0.40411 (train) and 0.02709 (test). The time used is 116.827s. 
At epoch 140/1200,the losses are 0.37475 (train) and 0.03835 (test). The time used is 116.759s. 
At epoch 160/1200,the losses are 0.34765 (train) and 0.02045 (test). The time used is 116.814s. 
At epoch 180/1200,the losses are 0.32797 (train) and 0.02318 (test). The time used is 116.855s. 
At epoch 200/1200,the losses are 0.30750 (train) and 0.01933 (test). The time used is 116.815s. 
At epoch 220/1200,the losses are 0.29252 (train) and 0.02398 (test). The time used is 116.860s. 
At epoch 240/1200,the losses are 0.27763 (train) and 0.01679 (test). The time used is 116.856s. 
At epoch 260/1200,the losses are 0.26435 (train) and 0.02849 (test). The time used is 116.805s. 
At epoch 280/1200,the losses are 0.25207 (train) and 0.01271 (test). The time used is 117.074s. 
Adjusting learning rate of group 0 to 1.0000e-04.
At epoch 300/1200,the losses are 0.24055 (train) and 0.01514 (test). The time used is 119.023s. 
At epoch 320/1200,the losses are 0.18003 (train) and 0.01080 (test). The time used is 120.229s. 
At epoch 340/1200,the losses are 0.17844 (train) and 0.01001 (test). The time used is 117.486s. 
At epoch 360/1200,the losses are 0.17656 (train) and 0.00976 (test). The time used is 116.772s. 
At epoch 380/1200,the losses are 0.17528 (train) and 0.01039 (test). The time used is 116.846s. 
At epoch 400/1200,the losses are 0.17409 (train) and 0.01060 (test). The time used is 116.776s. 
At epoch 420/1200,the losses are 0.17287 (train) and 0.00991 (test). The time used is 116.821s. 
At epoch 440/1200,the losses are 0.17194 (train) and 0.00975 (test). The time used is 116.765s. 
At epoch 460/1200,the losses are 0.17112 (train) and 0.00967 (test). The time used is 117.205s. 
At epoch 480/1200,the losses are 0.17035 (train) and 0.00931 (test). The time used is 116.847s. 
At epoch 500/1200,the losses are 0.16942 (train) and 0.00968 (test). The time used is 116.822s. 
At epoch 520/1200,the losses are 0.16898 (train) and 0.00973 (test). The time used is 116.785s. 
At epoch 540/1200,the losses are 0.16788 (train) and 0.00943 (test). The time used is 116.799s. 
At epoch 560/1200,the losses are 0.16717 (train) and 0.00899 (test). The time used is 116.835s. 
At epoch 580/1200,the losses are 0.16703 (train) and 0.00909 (test). The time used is 116.835s. 
Adjusting learning rate of group 0 to 1.0000e-05.
At epoch 600/1200,the losses are 0.16582 (train) and 0.00973 (test). The time used is 116.756s. 
At epoch 620/1200,the losses are 0.16028 (train) and 0.00868 (test). The time used is 116.756s. 
At epoch 640/1200,the losses are 0.16014 (train) and 0.00868 (test). The time used is 116.806s. 
At epoch 660/1200,the losses are 0.15984 (train) and 0.00866 (test). The time used is 116.780s. 
At epoch 680/1200,the losses are 0.15981 (train) and 0.00868 (test). The time used is 117.475s. 
At epoch 700/1200,the losses are 0.15972 (train) and 0.00875 (test). The time used is 118.022s. 
At epoch 720/1200,the losses are 0.15946 (train) and 0.00858 (test). The time used is 116.729s. 
At epoch 740/1200,the losses are 0.15946 (train) and 0.00862 (test). The time used is 117.488s. 
At epoch 760/1200,the losses are 0.15951 (train) and 0.00855 (test). The time used is 118.470s. 
At epoch 780/1200,the losses are 0.15926 (train) and 0.00865 (test). The time used is 116.759s. 
At epoch 800/1200,the losses are 0.15923 (train) and 0.00862 (test). The time used is 117.526s. 
At epoch 820/1200,the losses are 0.15915 (train) and 0.00866 (test). The time used is 120.268s. 
At epoch 840/1200,the losses are 0.15904 (train) and 0.00857 (test). The time used is 117.037s. 
At epoch 860/1200,the losses are 0.15895 (train) and 0.00859 (test). The time used is 116.759s. 
At epoch 880/1200,the losses are 0.15872 (train) and 0.00862 (test). The time used is 116.694s. 
Adjusting learning rate of group 0 to 1.0000e-06.
At epoch 900/1200,the losses are 0.15897 (train) and 0.00861 (test). The time used is 116.767s. 
At epoch 920/1200,the losses are 0.15823 (train) and 0.00851 (test). The time used is 116.756s. 
At epoch 940/1200,the losses are 0.15800 (train) and 0.00850 (test). The time used is 116.704s. 
At epoch 960/1200,the losses are 0.15831 (train) and 0.00850 (test). The time used is 116.733s. 
At epoch 980/1200,the losses are 0.15799 (train) and 0.00849 (test). The time used is 116.765s. 
At epoch 1000/1200,the losses are 0.15796 (train) and 0.00850 (test). The time used is 116.710s. 
At epoch 1020/1200,the losses are 0.15814 (train) and 0.00849 (test). The time used is 116.786s. 
At epoch 1040/1200,the losses are 0.15809 (train) and 0.00848 (test). The time used is 116.836s. 
At epoch 1060/1200,the losses are 0.15820 (train) and 0.00850 (test). The time used is 116.893s. 
At epoch 1080/1200,the losses are 0.15801 (train) and 0.00849 (test). The time used is 116.796s. 
At epoch 1100/1200,the losses are 0.15780 (train) and 0.00849 (test). The time used is 116.806s. 
At epoch 1120/1200,the losses are 0.15823 (train) and 0.00849 (test). The time used is 116.790s. 
At epoch 1140/1200,the losses are 0.15805 (train) and 0.00849 (test). The time used is 116.723s. 
At epoch 1160/1200,the losses are 0.15810 (train) and 0.00850 (test). The time used is 116.846s. 
At epoch 1180/1200,the losses are 0.15802 (train) and 0.00849 (test). The time used is 116.866s. 
Adjusting learning rate of group 0 to 1.0000e-07.
At epoch 1200/1200,the losses are 0.15769 (train) and 0.00850 (test). The time used is 117.160s. 
/data/rajlab1/user_data/jin/MyResearch/TV-SGM/bash_scripts/../mypkg/../results/SGM_net_large
Create a folder /data/rajlab1/user_data/jin/MyResearch/TV-SGM/bash_scripts/../mypkg/../results/SGM_net_large
Save to /data/rajlab1/user_data/jin/MyResearch/TV-SGM/bash_scripts/../mypkg/../results/SGM_net_large/model.pkl
Save to /data/rajlab1/user_data/jin/MyResearch/TV-SGM/bash_scripts/../mypkg/../results/SGM_net_large/loss.pkl
Save to /data/rajlab1/user_data/jin/MyResearch/TV-SGM/bash_scripts/../mypkg/../results/SGM_net_large/optimizer.pkl
Save to /data/rajlab1/user_data/jin/MyResearch/TV-SGM/bash_scripts/../mypkg/../results/SGM_net_large/paras.pkl
Save to /data/rajlab1/user_data/jin/MyResearch/TV-SGM/bash_scripts/../mypkg/../results/SGM_net_large/loss_test.pkl
Save to /data/rajlab1/user_data/jin/MyResearch/TV-SGM/bash_scripts/../mypkg/../results/SGM_net_large/freqs.pkl
